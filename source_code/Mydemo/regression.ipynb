{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 crescent_gap, 2 welding_lines, 1: 640x640 1 water_spot, 2: 640x640 2 oil_spots, 3: 640x640 1 water_spot, 4: 640x640 4 crescent_gaps, 1 silk_spot, 1 welding_line, 5: 640x640 1 silk_spot, 6: 640x640 3 oil_spots, 3 punching_holes, 1 waist_folding, 7: 640x640 8 water_spots, 8: 640x640 2 waist_foldings, 9: 640x640 1 waist_folding, 10: 640x640 1 crescent_gap, 2 welding_lines, 11: 640x640 1 crescent_gap, 12: 640x640 (no detections), 13: 640x640 1 silk_spot, 14: 640x640 (no detections), 15: 640x640 2 creases, 16: 640x640 1 oil_spot, 17: 640x640 2 water_spots, 18: 640x640 4 oil_spots, 19: 640x640 1 crescent_gap, 1 welding_line, 20: 640x640 3 crescent_gaps, 1 welding_line, 21: 640x640 (no detections), 22: 640x640 1 punching_hole, 23: 640x640 3 welding_lines, 24: 640x640 2 punching_holes, 25: 640x640 1 silk_spot, 26: 640x640 2 punching_holes, 1 welding_line, 27: 640x640 1 water_spot, 28: 640x640 1 water_spot, 29: 640x640 (no detections), 30: 640x640 1 water_spot, 31: 640x640 1 water_spot, 32: 640x640 2 silk_spots, 33: 640x640 1 punching_hole, 34: 640x640 3 welding_lines, 35: 640x640 1 welding_line, 36: 640x640 1 water_spot, 37: 640x640 1 punching_hole, 38: 640x640 2 waist_foldings, 39: 640x640 (no detections), 40: 640x640 1 silk_spot, 41: 640x640 (no detections), 42: 640x640 1 punching_hole, 2 silk_spots, 43: 640x640 (no detections), 44: 640x640 1 silk_spot, 45: 640x640 (no detections), 46: 640x640 1 welding_line, 47: 640x640 1 punching_hole, 1 welding_line, 48: 640x640 1 water_spot, 49: 640x640 (no detections), 50: 640x640 2 oil_spots, 51: 640x640 1 silk_spot, 52: 640x640 1 waist_folding, 53: 640x640 4 oil_spots, 54: 640x640 1 crescent_gap, 55: 640x640 1 punching_hole, 1 welding_line, 56: 640x640 1 water_spot, 57: 640x640 (no detections), 58: 640x640 1 welding_line, 59: 640x640 1 crescent_gap, 1 welding_line, 60: 640x640 2 water_spots, 61: 640x640 1 water_spot, 62: 640x640 1 silk_spot, 63: 640x640 4 silk_spots, 64: 640x640 1 waist_folding, 65: 640x640 1 water_spot, 66: 640x640 1 inclusion, 67: 640x640 2 oil_spots, 2 water_spots, 68: 640x640 2 oil_spots, 69: 640x640 (no detections), 70: 640x640 (no detections), 71: 640x640 (no detections), 72: 640x640 1 silk_spot, 73: 640x640 1 crescent_gap, 2 welding_lines, 74: 640x640 1 oil_spot, 75: 640x640 1 silk_spot, 76: 640x640 2 water_spots, 77: 640x640 1 silk_spot, 78: 640x640 1 silk_spot, 79: 640x640 1 waist_folding, 80: 640x640 (no detections), 81: 640x640 1 crescent_gap, 1 welding_line, 82: 640x640 (no detections), 83: 640x640 1 crescent_gap, 1 welding_line, 84: 640x640 1 water_spot, 85: 640x640 1 crescent_gap, 1 punching_hole, 1 welding_line, 86: 640x640 (no detections), 87: 640x640 (no detections), 88: 640x640 (no detections), 89: 640x640 1 crescent_gap, 1 welding_line, 90: 640x640 1 water_spot, 91: 640x640 (no detections), 92: 640x640 1 crescent_gap, 1 silk_spot, 1 welding_line, 93: 640x640 1 oil_spot, 94: 640x640 2 crescent_gaps, 95: 640x640 1 punching_hole, 96: 640x640 1 water_spot, 97: 640x640 2 silk_spots, 98: 640x640 1 punching_hole, 1 silk_spot, 1 waist_folding, 2 welding_lines, 99: 640x640 1 oil_spot, 100: 640x640 1 silk_spot, 101: 640x640 (no detections), 102: 640x640 1 crescent_gap, 1 silk_spot, 103: 640x640 1 silk_spot, 3 welding_lines, 104: 640x640 1 silk_spot, 105: 640x640 1 water_spot, 106: 640x640 (no detections), 107: 640x640 2 welding_lines, 108: 640x640 1 oil_spot, 109: 640x640 1 silk_spot, 110: 640x640 1 waist_folding, 111: 640x640 2 welding_lines, 112: 640x640 1 crescent_gap, 113: 640x640 1 waist_folding, 114: 640x640 1 silk_spot, 115: 640x640 (no detections), 116: 640x640 2 water_spots, 117: 640x640 1 punching_hole, 3 welding_lines, 118: 640x640 1 silk_spot, 119: 640x640 1 crescent_gap, 1 welding_line, 120: 640x640 3 silk_spots, 121: 640x640 2 water_spots, 122: 640x640 1 water_spot, 123: 640x640 2 silk_spots, 124: 640x640 1 silk_spot, 125: 640x640 (no detections), 126: 640x640 1 water_spot, 127: 640x640 (no detections), 128: 640x640 1 silk_spot, 129: 640x640 1 silk_spot, 130: 640x640 1 punching_hole, 131: 640x640 2 crescent_gaps, 2 oil_spots, 2 welding_lines, 132: 640x640 1 oil_spot, 133: 640x640 4 oil_spots, 134: 640x640 1 silk_spot, 135: 640x640 1 waist_folding, 136: 640x640 1 crescent_gap, 1 oil_spot, 137: 640x640 1 punching_hole, 1 welding_line, 138: 640x640 1 water_spot, 139: 640x640 (no detections), 140: 640x640 2 water_spots, 141: 640x640 2 crescent_gaps, 142: 640x640 1 silk_spot, 143: 640x640 1 silk_spot, 144: 640x640 2 silk_spots, 145: 640x640 1 silk_spot, 146: 640x640 1 silk_spot, 147: 640x640 1 waist_folding, 148: 640x640 (no detections), 149: 640x640 (no detections), 150: 640x640 (no detections), 151: 640x640 1 silk_spot, 152: 640x640 2 silk_spots, 153: 640x640 1 silk_spot, 154: 640x640 2 silk_spots, 155: 640x640 1 silk_spot, 156: 640x640 2 silk_spots, 157: 640x640 (no detections), 158: 640x640 1 welding_line, 159: 640x640 1 crescent_gap, 160: 640x640 1 crescent_gap, 1 welding_line, 161: 640x640 1 waist_folding, 162: 640x640 1 silk_spot, 163: 640x640 1 welding_line, 164: 640x640 2 punching_holes, 2 welding_lines, 165: 640x640 1 crescent_gap, 1 welding_line, 166: 640x640 2 punching_holes, 1 welding_line, 167: 640x640 1 oil_spot, 168: 640x640 1 silk_spot, 169: 640x640 1 oil_spot, 170: 640x640 1 silk_spot, 171: 640x640 1 water_spot, 172: 640x640 1 punching_hole, 173: 640x640 1 silk_spot, 174: 640x640 1 silk_spot, 175: 640x640 (no detections), 176: 640x640 1 silk_spot, 177: 640x640 1 punching_hole, 2 welding_lines, 178: 640x640 2 water_spots, 179: 640x640 1 punching_hole, 1 welding_line, 180: 640x640 (no detections), 181: 640x640 1 crescent_gap, 1 welding_line, 182: 640x640 (no detections), 183: 640x640 2 welding_lines, 184: 640x640 2 crescent_gaps, 1 welding_line, 185: 640x640 (no detections), 186: 640x640 1 water_spot, 187: 640x640 1 oil_spot, 188: 640x640 2 water_spots, 189: 640x640 1 crescent_gap, 1 oil_spot, 1 water_spot, 190: 640x640 3 waist_foldings, 191: 640x640 1 punching_hole, 192: 640x640 1 silk_spot, 193: 640x640 1 silk_spot, 194: 640x640 1 silk_spot, 195: 640x640 (no detections), 196: 640x640 1 crescent_gap, 197: 640x640 1 welding_line, 198: 640x640 1 water_spot, 199: 640x640 1 crescent_gap, 1 welding_line, 200: 640x640 1 punching_hole, 201: 640x640 1 crescent_gap, 2 welding_lines, 202: 640x640 (no detections), 203: 640x640 (no detections), 204: 640x640 1 crescent_gap, 205: 640x640 1 oil_spot, 1 silk_spot, 206: 640x640 1 punching_hole, 207: 640x640 (no detections), 208: 640x640 2 welding_lines, 209: 640x640 1 punching_hole, 210: 640x640 1 welding_line, 211: 640x640 1 silk_spot, 1 water_spot, 212: 640x640 1 crescent_gap, 2 welding_lines, 213: 640x640 1 crescent_gap, 1 welding_line, 214: 640x640 1 water_spot, 215: 640x640 (no detections), 216: 640x640 (no detections), 217: 640x640 (no detections), 218: 640x640 1 water_spot, 219: 640x640 1 oil_spot, 220: 640x640 1 punching_hole, 3 welding_lines, 221: 640x640 1 silk_spot, 222: 640x640 2 silk_spots, 223: 640x640 (no detections), 224: 640x640 (no detections), 225: 640x640 (no detections), 226: 640x640 1 water_spot, 227: 640x640 (no detections), 228: 640x640 1 punching_hole, 229: 640x640 3 waist_foldings, 510.1ms\n",
      "Speed: 2.4ms preprocess, 2.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "#set visible cuda\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_brightness_0 detect by yolov8n with dropout(p=0.1)/weights/best.pt')\n",
    "\n",
    "# Define path to directory containing images and videos for inference\n",
    "source = '/Data4/student_zhihan_data/data/GC10-DET/test/images'\n",
    "\n",
    "\n",
    "# Run inference on the source\n",
    "results = model([os.path.join(source, i) for i in os.listdir(source)])# generator of Results objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import compute_ap\n",
    "from ultralytics.engine.validator import BaseValidator\n",
    "from ultralytics.utils.metrics import box_iou, Metric, DetMetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def smooth(y, f=0.05):\n",
    "    \"\"\"Box filter of fraction f.\"\"\"\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = np.ones(nf // 2)  # ones padding\n",
    "    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return np.convolve(yp, np.ones(nf) / nf, mode=\"valid\")  # y-smoothed\n",
    "\n",
    "def ap_per_class(\n",
    "    tp, conf, pred_cls, target_cls, plot=False, on_plot=None, save_dir=Path(), names=(), eps=1e-16, prefix=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the average precision per class for object detection evaluation.\n",
    "\n",
    "    Args:\n",
    "        tp (np.ndarray): Binary array indicating whether the detection is correct (True) or not (False).\n",
    "        conf (np.ndarray): Array of confidence scores of the detections.\n",
    "        pred_cls (np.ndarray): Array of predicted classes of the detections.\n",
    "        target_cls (np.ndarray): Array of true classes of the detections.\n",
    "        plot (bool, optional): Whether to plot PR curves or not. Defaults to False.\n",
    "        on_plot (func, optional): A callback to pass plots path and data when they are rendered. Defaults to None.\n",
    "        save_dir (Path, optional): Directory to save the PR curves. Defaults to an empty path.\n",
    "        names (tuple, optional): Tuple of class names to plot PR curves. Defaults to an empty tuple.\n",
    "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-16.\n",
    "        prefix (str, optional): A prefix string for saving the plot files. Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): A tuple of six arrays and one array of unique classes, where:\n",
    "            tp (np.ndarray): True positive counts at threshold given by max F1 metric for each class.Shape: (nc,).\n",
    "            fp (np.ndarray): False positive counts at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            p (np.ndarray): Precision values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            r (np.ndarray): Recall values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            f1 (np.ndarray): F1-score values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            ap (np.ndarray): Average precision for each class at different IoU thresholds. Shape: (nc, 10).\n",
    "            unique_classes (np.ndarray): An array of unique classes that have data. Shape: (nc,).\n",
    "            p_curve (np.ndarray): Precision curves for each class. Shape: (nc, 1000).\n",
    "            r_curve (np.ndarray): Recall curves for each class. Shape: (nc, 1000).\n",
    "            f1_curve (np.ndarray): F1-score curves for each class. Shape: (nc, 1000).\n",
    "            x (np.ndarray): X-axis values for the curves. Shape: (1000,).\n",
    "            prec_values: Precision values at mAP@0.5 for each class. Shape: (nc, 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    x, prec_values = np.linspace(0, 1, 1000), []\n",
    "\n",
    "    # Average precision, precision and recall curves\n",
    "    ap, p_curve, r_curve = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_l = nt[ci]  # number of labels\n",
    "        n_p = i.sum()  # number of predictions\n",
    "        if n_p == 0 or n_l == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (n_l + eps)  # recall curve\n",
    "        r_curve[ci] = np.interp(-x, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p_curve[ci] = np.interp(-x, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
    "            if plot and j == 0:\n",
    "                prec_values.append(np.interp(x, mrec, mpre))  # precision at mAP@0.5\n",
    "\n",
    "    prec_values = np.array(prec_values)  # (nc, 1000)\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1_curve = 2 * p_curve * r_curve / (p_curve + r_curve + eps)\n",
    "    f2_curve = (5 * p_curve * r_curve) / (4 * p_curve + r_curve + eps)\n",
    "    # names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n",
    "    # names = dict(enumerate(names))  # to dict\n",
    "    # if plot:\n",
    "    #     plot_pr_curve(x, prec_values, ap, save_dir / f\"{prefix}PR_curve.png\", names, on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, f1_curve, save_dir / f\"{prefix}F1_curve.png\", names, ylabel=\"F1\", on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, p_curve, save_dir / f\"{prefix}P_curve.png\", names, ylabel=\"Precision\", on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, r_curve, save_dir / f\"{prefix}R_curve.png\", names, ylabel=\"Recall\", on_plot=on_plot)\n",
    "\n",
    "    i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1, f2 = p_curve[:, i], r_curve[:, i], f1_curve[:, i], f2_curve[:, i]  # max-F1 precision, recall, F1 values\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    return tp, fp, p, r, f1, f2, ap, unique_classes.astype(int), p_curve, r_curve, f1_curve, f2_curve, x, prec_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f2(source, model):\n",
    "    results = model([os.path.join(source, i) for i in os.listdir(source)])\n",
    "    validator = BaseValidator()\n",
    "    validator.iouv = torch.arange(0.5, 1, 0.05)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['img_name', 'p', 'r', 'f1', 'f2', 'conf', 'pred_cls', 'target_cls'])\n",
    "\n",
    "    for result in results:\n",
    "        metric = DetMetrics()\n",
    "        label_path = result.path.replace('images', 'labels')[:-4] + '.txt'\n",
    "        label = torch.from_numpy(np.loadtxt(label_path))\n",
    "        # turn label into (x1, y1, x2, y2) format\n",
    "        \n",
    "        if len(label) == 0:\n",
    "            continue\n",
    "        \n",
    "        if label.dim() > 1:\n",
    "            x1 = label[:, 1] - label[:, 3] / 2\n",
    "            y1 = label[:, 2] - label[:, 4] / 2\n",
    "            x2 = label[:, 1] + label[:, 3] / 2\n",
    "            y2 = label[:, 2] + label[:, 4] / 2\n",
    "            label = torch.stack((label[:, 0], x1, y1, x2, y2), 1)\n",
    "            iou = box_iou(label[:, 1:].to(\"cuda:0\"), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "            tp = validator.match_predictions(result.boxes.data[:,-1], label[:, 0].to(\"cuda:0\"), iou)\n",
    "            \n",
    "            # update metric\n",
    "            tp = tp.detach().cpu().numpy()\n",
    "            conf = result.boxes.conf.detach().cpu().numpy()\n",
    "            pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "            target_cls = label[:, 0].detach().cpu().numpy()\n",
    "        \n",
    "        elif label.dim() == 1:\n",
    "            x1 = label[1] - label[3] / 2\n",
    "            y1 = label[2] - label[4] / 2\n",
    "            x2 = label[1] + label[3] / 2\n",
    "            y2 = label[2] + label[4] / 2\n",
    "            label = torch.tensor([label[0], x1, y1, x2, y2])\n",
    "            iou = box_iou(label[1:].to(\"cuda:0\").reshape(1, -1), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "            tp = validator.match_predictions(result.boxes.data[:,-1], label[0].to(\"cuda:0\").unsqueeze(0), iou)\n",
    "        \n",
    "            # update metric\n",
    "            tp = tp.detach().cpu().numpy()\n",
    "            conf = result.boxes.conf.detach().cpu().numpy()\n",
    "            pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "            target_cls = label[0].unsqueeze(0).detach().cpu().numpy()\n",
    "            \n",
    "        _, _, p, r, f1, f2, ap, unique_classes, p_curve, r_curve, f1_curve, f2_curve, x, prec_values = ap_per_class(tp, conf, pre_cls, target_cls)\n",
    "        print(f2.mean(), unique_classes)\n",
    "        \n",
    "        # update df\n",
    "        df.loc[len(df)] = [result.path, p, r, f1, f2, conf, pre_cls, target_cls]\n",
    "        \n",
    "    df.to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record/{source.split(\"/\")[-3]}.csv', index=False, header=True)\n",
    "    return df\n",
    "  \n",
    "\n",
    "# for i in os.listdir('/Data4/student_zhihan_data/data'):\n",
    "#     if i[-3:] != 'csv' and i != 'NEU-DET' and i != 'data.zip':\n",
    "#         source = os.path.join('/Data4/student_zhihan_data/data', i, 'test/images')\n",
    "#         df = compute_f2(source, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: uncertainty * 3, proposed * score, NIQE, BRISQUE\n",
    "# output: quality score \n",
    "\n",
    "F2_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record'\n",
    "Proposed_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Score_Record'\n",
    "Uncertainty_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record'\n",
    "\n",
    "# concat all csv\n",
    "dfs = []\n",
    "for idx, dir in enumerate([F2_dir, Proposed_dir, Uncertainty_dir]):\n",
    "    files = os.listdir(dir)\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        if file[-3:] == 'csv':\n",
    "            df = pd.concat([df, pd.read_csv(os.path.join(dir, file))], ignore_index=True)\n",
    "    if idx == 0:       \n",
    "        df.to_csv(f'{dir}.csv', index=False, header=True)\n",
    "    else:\n",
    "        df['img_name'] = df['dataset'] + '/images/' + df['img_name']\n",
    "        df = df.drop(columns=['dataset'])\n",
    "        df.to_csv(f'{dir}.csv', index=False, header=True)\n",
    "    dfs.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2347512/2492081121.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, tmp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(Proposed_dir)\n",
    "df = pd.DataFrame(columns=['dataset', 'img_name', 'visibility', 'exposure'])\n",
    "for file in files:\n",
    "    if file[-3:] == 'csv':\n",
    "        tmp = pd.read_csv(os.path.join(Proposed_dir, file))\n",
    "        #add header to tmp\n",
    "        tmp.columns = ['dataset', 'img_name', 'visibility', 'exposure']\n",
    "        df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "df.to_csv(f'Proposed_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for idx, dir in enumerate(['/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record.csv',\n",
    "                          '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv',\n",
    "                           '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv']):\n",
    "    df = pd.read_csv(dir)\n",
    "    dfs.append(df)\n",
    " \n",
    "merged_df = pd.merge(dfs[0], dfs[1], on=['img_name'])\n",
    "merged_df = pd.merge(merged_df, dfs[2], on=['img_name'])\n",
    "    \n",
    "    \n",
    "# columns = set()\n",
    "# for df in dfs:\n",
    "#     columns = columns.union(set(df.columns))\n",
    "\n",
    "# df = pd.DataFrame(columns=list(columns))\n",
    "# img_name_union = set(dfs[0]['img_name'])\n",
    "\n",
    "# for tmp in dfs:\n",
    "#     img_name_union = img_name_union.intersection(set(tmp['img_name']))\n",
    "\n",
    "# for tmp in dfs:\n",
    "#     merged_df = pd.merge(df, tmp, on=['img_name'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv')\n",
    "a =  df['img_name'].apply(lambda x: '/'.join(x.split('/')[:-2]))\n",
    "b = df['img_name'].apply(lambda x: '/'.join(x.split('/')[-2:]))\n",
    "\n",
    "#concat a, '/images/, b to a new column\n",
    "df['img_name'] = a + '/test/' + b\n",
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv')\n",
    "df['img_name'] = df['dataset'] + '/images/' + df['img_name']\n",
    "df = df.drop(columns=['dataset'])\n",
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.validator import BaseValidator\n",
    "from ultralytics.utils.metrics import box_iou, Metric, DetMetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# load txt label\n",
    "\n",
    "\n",
    "validator = BaseValidator()\n",
    "validator.iouv = torch.arange(0.5, 1, 0.05)\n",
    "\n",
    "for result in results:\n",
    "    metric = DetMetrics()\n",
    "    label_path = result.path.replace('images', 'labels')[:-4] + '.txt'\n",
    "    label = torch.from_numpy(np.loadtxt(label_path))\n",
    "    # turn label into (x1, y1, x2, y2) format\n",
    "    \n",
    "    if label.dim() > 1:\n",
    "        x1 = label[:, 1] - label[:, 3] / 2\n",
    "        y1 = label[:, 2] - label[:, 4] / 2\n",
    "        x2 = label[:, 1] + label[:, 3] / 2\n",
    "        y2 = label[:, 2] + label[:, 4] / 2\n",
    "        label = torch.stack((label[:, 0], x1, y1, x2, y2), 1)\n",
    "        iou = box_iou(label[:, 1:].to(\"cuda:0\"), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "        tp = validator.match_predictions(result.boxes.data[:,-1], label[:, 0].to(\"cuda:0\"), iou)\n",
    "        \n",
    "        # update metric\n",
    "        tp = tp.detach().cpu().numpy()\n",
    "        conf = result.boxes.conf.detach().cpu().numpy()\n",
    "        pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "        target_cls = label[:, 0].detach().cpu().numpy()\n",
    "    \n",
    "    else:\n",
    "        x1 = label[1] - label[3] / 2\n",
    "        y1 = label[2] - label[4] / 2\n",
    "        x2 = label[1] + label[3] / 2\n",
    "        y2 = label[2] + label[4] / 2\n",
    "        label = torch.tensor([label[0], x1, y1, x2, y2])\n",
    "        iou = box_iou(label[1:].to(\"cuda:0\").reshape(1, -1), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "        tp = validator.match_predictions(result.boxes.data[:,-1], label[0].to(\"cuda:0\").unsqueeze(0), iou)\n",
    "    \n",
    "        # update metric\n",
    "        tp = tp.detach().cpu().numpy()\n",
    "        conf = result.boxes.conf.detach().cpu().numpy()\n",
    "        pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "        target_cls = label[0].unsqueeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    _, _, p, r, f1, f2, ap, unique_classes, p_curve, r_curve, f1_curve, f2_curve, x, prec_values = ap_per_class(tp, conf, pre_cls, target_cls)\n",
    "    print(f2.mean(), unique_classes)\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results generator\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bbox outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533106</td>\n",
       "      <td>0.034241</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>0.158915</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.694677</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.120407</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.354166</td>\n",
       "      <td>0.152297</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.113457</td>\n",
       "      <td>0.016292</td>\n",
       "      <td>0.082116</td>\n",
       "      <td>0.000533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165364</td>\n",
       "      <td>0.105627</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.091819</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visibility  exposure  objectness_uncertainty  weighted_variance_sum  \\\n",
       "0    0.533106  0.034241                0.003777               0.158915   \n",
       "1    0.694677  0.001499                0.009511               0.120407   \n",
       "2    0.354166  0.152297                0.009658               0.061321   \n",
       "3    0.006219  0.113457                0.016292               0.082116   \n",
       "4    0.165364  0.105627                0.002982               0.091819   \n",
       "\n",
       "   weighted_entropy  \n",
       "0          0.000058  \n",
       "1          0.000020  \n",
       "2          0.000055  \n",
       "3          0.000533  \n",
       "4          0.000116  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def safe_convert_to_list(s):\n",
    "    try:\n",
    "        # Attempt to directly evaluate the string\n",
    "        return ast.literal_eval(s)\n",
    "    except SyntaxError:\n",
    "        # If direct evaluation fails, attempt to manually parse the string\n",
    "        cleaned_str = s.strip('[]')\n",
    "        if cleaned_str:  # Check if the string is not empty\n",
    "            numbers = [float(num) for num in cleaned_str.split() if num not in ['[', ']']]\n",
    "            return numbers\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "data.head()\n",
    "\n",
    "# Apply the conversion function to the 'f2' column and then compute the mean F2 score\n",
    "data['f2'] = data['f2'].apply(safe_convert_to_list)\n",
    "data['mean_f2'] = data['f2'].apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "\n",
    "# Prepare the dataset for modeling\n",
    "features = ['visibility', 'exposure', 'objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy']\n",
    "X = data[features]\n",
    "y = data['mean_f2']\n",
    "\n",
    "# Display the first few rows of features and target to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.188070</td>\n",
       "      <td>0.107889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Regressor</th>\n",
       "      <td>0.275943</td>\n",
       "      <td>-0.308935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP Regressor</th>\n",
       "      <td>0.185389</td>\n",
       "      <td>0.120607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regressor</th>\n",
       "      <td>0.140655</td>\n",
       "      <td>0.332804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MSE        R2\n",
       "Linear Regression        0.188070  0.107889\n",
       "Decision Tree Regressor  0.275943 -0.308935\n",
       "MLP Regressor            0.185389  0.120607\n",
       "Random Forest Regressor  0.140655  0.332804"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"MLP Regressor\": MLPRegressor(random_state=42, max_iter=1000), # Increased max_iter for convergence\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\"MSE\": mse, \"R2\": r2}\n",
    "    \n",
    "    if name == 'Decision Tree Regressor':\n",
    "        # save test resulst to csv\n",
    "        df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "        df.to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/test_result.csv', index=False)\n",
    "\n",
    "results_df = pd.DataFrame(results).T  # Convert results to a DataFrame for better readability\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.16305,     0.16094,     0.16593,      0.2284,     0.28168])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.867000e+03</td>\n",
       "      <td>4867.000000</td>\n",
       "      <td>4.867000e+03</td>\n",
       "      <td>4867.000000</td>\n",
       "      <td>4867.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.796616e-01</td>\n",
       "      <td>0.232434</td>\n",
       "      <td>6.412672e-03</td>\n",
       "      <td>0.144884</td>\n",
       "      <td>0.001363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.144880e-01</td>\n",
       "      <td>0.268423</td>\n",
       "      <td>4.923872e-03</td>\n",
       "      <td>0.072237</td>\n",
       "      <td>0.004153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.222425e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.274079e-07</td>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.034640e-01</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>2.396813e-03</td>\n",
       "      <td>0.096459</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.356180e-01</td>\n",
       "      <td>0.119043</td>\n",
       "      <td>5.491689e-03</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.143271e-01</td>\n",
       "      <td>0.384620</td>\n",
       "      <td>9.171929e-03</td>\n",
       "      <td>0.203721</td>\n",
       "      <td>0.000790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.695124e-01</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>2.989884e-02</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.056565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         visibility     exposure  objectness_uncertainty  \\\n",
       "count  4.867000e+03  4867.000000            4.867000e+03   \n",
       "mean   2.796616e-01     0.232434            6.412672e-03   \n",
       "std    2.144880e-01     0.268423            4.923872e-03   \n",
       "min    3.222425e-07     0.000000            1.274079e-07   \n",
       "25%    1.034640e-01     0.016136            2.396813e-03   \n",
       "50%    2.356180e-01     0.119043            5.491689e-03   \n",
       "75%    4.143271e-01     0.384620            9.171929e-03   \n",
       "max    9.695124e-01     0.999207            2.989884e-02   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy  \n",
       "count            4867.000000       4867.000000  \n",
       "mean                0.144884          0.001363  \n",
       "std                 0.072237          0.004153  \n",
       "min                 0.050507          0.000000  \n",
       "25%                 0.096459          0.000037  \n",
       "50%                 0.105285          0.000154  \n",
       "75%                 0.203721          0.000790  \n",
       "max                 0.479202          0.056565  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1.217000e+03</td>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1217.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.270493</td>\n",
       "      <td>0.230253</td>\n",
       "      <td>6.240577e-03</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.001661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.209810</td>\n",
       "      <td>0.272485</td>\n",
       "      <td>4.841622e-03</td>\n",
       "      <td>0.073080</td>\n",
       "      <td>0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.902979e-08</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.093177</td>\n",
       "      <td>0.010945</td>\n",
       "      <td>2.279311e-03</td>\n",
       "      <td>0.097146</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.230777</td>\n",
       "      <td>0.109753</td>\n",
       "      <td>5.438593e-03</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.402377</td>\n",
       "      <td>0.393562</td>\n",
       "      <td>8.849673e-03</td>\n",
       "      <td>0.226471</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.958951</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>2.272954e-02</td>\n",
       "      <td>0.413786</td>\n",
       "      <td>0.040970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        visibility     exposure  objectness_uncertainty  \\\n",
       "count  1217.000000  1217.000000            1.217000e+03   \n",
       "mean      0.270493     0.230253            6.240577e-03   \n",
       "std       0.209810     0.272485            4.841622e-03   \n",
       "min       0.000420     0.000000            3.902979e-08   \n",
       "25%       0.093177     0.010945            2.279311e-03   \n",
       "50%       0.230777     0.109753            5.438593e-03   \n",
       "75%       0.402377     0.393562            8.849673e-03   \n",
       "max       0.958951     0.999026            2.272954e-02   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy  \n",
       "count            1217.000000       1217.000000  \n",
       "mean                0.149462          0.001661  \n",
       "std                 0.073080          0.004813  \n",
       "min                 0.055160          0.000000  \n",
       "25%                 0.097146          0.000037  \n",
       "50%                 0.107024          0.000173  \n",
       "75%                 0.226471          0.000954  \n",
       "max                 0.413786          0.040970  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4867.000000\n",
       "mean        0.476630\n",
       "std         0.462209\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: mean_f2, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1217.000000\n",
       "mean        0.474405\n",
       "std         0.459334\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: mean_f2, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
