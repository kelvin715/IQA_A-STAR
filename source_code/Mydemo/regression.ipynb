{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "#set visible cuda\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_brightness_0 detect by yolov8n with dropout(p=0.1)/weights/best.pt')\n",
    "\n",
    "# Define path to directory containing images and videos for inference\n",
    "# source = '/Data4/student_zhihan_data/data/GC10-DET/test/images'\n",
    "\n",
    "# # Run inference on the source\n",
    "# results = model([os.path.join(source, i) for i in os.listdir(source)])# generator of Results objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import compute_ap\n",
    "from ultralytics.engine.validator import BaseValidator\n",
    "from ultralytics.utils.metrics import box_iou, Metric, DetMetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def smooth(y, f=0.05):\n",
    "    \"\"\"Box filter of fraction f.\"\"\"\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = np.ones(nf // 2)  # ones padding\n",
    "    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return np.convolve(yp, np.ones(nf) / nf, mode=\"valid\")  # y-smoothed\n",
    "\n",
    "def ap_per_class(\n",
    "    tp, conf, pred_cls, target_cls, plot=False, on_plot=None, save_dir=Path(), names=(), eps=1e-16, prefix=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the average precision per class for object detection evaluation.\n",
    "\n",
    "    Args:\n",
    "        tp (np.ndarray): Binary array indicating whether the detection is correct (True) or not (False).\n",
    "        conf (np.ndarray): Array of confidence scores of the detections.\n",
    "        pred_cls (np.ndarray): Array of predicted classes of the detections.\n",
    "        target_cls (np.ndarray): Array of true classes of the detections.\n",
    "        plot (bool, optional): Whether to plot PR curves or not. Defaults to False.\n",
    "        on_plot (func, optional): A callback to pass plots path and data when they are rendered. Defaults to None.\n",
    "        save_dir (Path, optional): Directory to save the PR curves. Defaults to an empty path.\n",
    "        names (tuple, optional): Tuple of class names to plot PR curves. Defaults to an empty tuple.\n",
    "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-16.\n",
    "        prefix (str, optional): A prefix string for saving the plot files. Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): A tuple of six arrays and one array of unique classes, where:\n",
    "            tp (np.ndarray): True positive counts at threshold given by max F1 metric for each class.Shape: (nc,).\n",
    "            fp (np.ndarray): False positive counts at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            p (np.ndarray): Precision values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            r (np.ndarray): Recall values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            f1 (np.ndarray): F1-score values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            ap (np.ndarray): Average precision for each class at different IoU thresholds. Shape: (nc, 10).\n",
    "            unique_classes (np.ndarray): An array of unique classes that have data. Shape: (nc,).\n",
    "            p_curve (np.ndarray): Precision curves for each class. Shape: (nc, 1000).\n",
    "            r_curve (np.ndarray): Recall curves for each class. Shape: (nc, 1000).\n",
    "            f1_curve (np.ndarray): F1-score curves for each class. Shape: (nc, 1000).\n",
    "            x (np.ndarray): X-axis values for the curves. Shape: (1000,).\n",
    "            prec_values: Precision values at mAP@0.5 for each class. Shape: (nc, 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    x, prec_values = np.linspace(0, 1, 1000), []\n",
    "\n",
    "    # Average precision, precision and recall curves\n",
    "    ap, p_curve, r_curve = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_l = nt[ci]  # number of labels\n",
    "        n_p = i.sum()  # number of predictions\n",
    "        if n_p == 0 or n_l == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (n_l + eps)  # recall curve\n",
    "        r_curve[ci] = np.interp(-x, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p_curve[ci] = np.interp(-x, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
    "            if plot and j == 0:\n",
    "                prec_values.append(np.interp(x, mrec, mpre))  # precision at mAP@0.5\n",
    "\n",
    "    prec_values = np.array(prec_values)  # (nc, 1000)\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1_curve = 2 * p_curve * r_curve / (p_curve + r_curve + eps)\n",
    "    f2_curve = (5 * p_curve * r_curve) / (4 * p_curve + r_curve + eps)\n",
    "    # names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n",
    "    # names = dict(enumerate(names))  # to dict\n",
    "    # if plot:\n",
    "    #     plot_pr_curve(x, prec_values, ap, save_dir / f\"{prefix}PR_curve.png\", names, on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, f1_curve, save_dir / f\"{prefix}F1_curve.png\", names, ylabel=\"F1\", on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, p_curve, save_dir / f\"{prefix}P_curve.png\", names, ylabel=\"Precision\", on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, r_curve, save_dir / f\"{prefix}R_curve.png\", names, ylabel=\"Recall\", on_plot=on_plot)\n",
    "\n",
    "    i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1, f2 = p_curve[:, i], r_curve[:, i], f1_curve[:, i], f2_curve[:, i]  # max-F1 precision, recall, F1 values\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    \n",
    "    ap_50 = ap[:, 0].mean()\n",
    "    ap_50_95 = ap.mean()\n",
    "    \n",
    "    return tp, fp, p, r, f1, f2, ap_50, ap_50_95, unique_classes.astype(int), p_curve, r_curve, f1_curve, f2_curve, x, prec_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f2(source, model):\n",
    "    images = os.listdir(source)\n",
    "    results = []\n",
    "    size = len(images) // 10\n",
    "    for i in range(11):\n",
    "        images_ = images[i* size: (i+1) * size] if i < 10 else images[i * size:]\n",
    "        results.extend(model([os.path.join(source, i) for i in images_]))\n",
    "\n",
    "    validator = BaseValidator()\n",
    "    validator.iouv = torch.arange(0.5, 1, 0.05)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['img_name', 'p', 'r', 'f1', 'f2', 'ap_50', 'ap_50_95', 'conf', 'pred_cls', 'target_cls'])\n",
    "\n",
    "    for result in results:\n",
    "        metric = DetMetrics()\n",
    "        label_path = result.path.replace('images', 'labels')[:-4] + '.txt'\n",
    "        label = torch.from_numpy(np.loadtxt(label_path))\n",
    "        # turn label into (x1, y1, x2, y2) format\n",
    "        \n",
    "        if len(label) == 0:\n",
    "            continue\n",
    "        \n",
    "        if label.dim() > 1:\n",
    "            x1 = label[:, 1] - label[:, 3] / 2\n",
    "            y1 = label[:, 2] - label[:, 4] / 2\n",
    "            x2 = label[:, 1] + label[:, 3] / 2\n",
    "            y2 = label[:, 2] + label[:, 4] / 2\n",
    "            label = torch.stack((label[:, 0], x1, y1, x2, y2), 1)\n",
    "            iou = box_iou(label[:, 1:].to(\"cuda:0\"), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "            tp = validator.match_predictions(result.boxes.data[:,-1], label[:, 0].to(\"cuda:0\"), iou)\n",
    "            \n",
    "            # update metric\n",
    "            tp = tp.detach().cpu().numpy()\n",
    "            conf = result.boxes.conf.detach().cpu().numpy()\n",
    "            pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "            target_cls = label[:, 0].detach().cpu().numpy()\n",
    "        \n",
    "        elif label.dim() == 1:\n",
    "            x1 = label[1] - label[3] / 2\n",
    "            y1 = label[2] - label[4] / 2\n",
    "            x2 = label[1] + label[3] / 2\n",
    "            y2 = label[2] + label[4] / 2\n",
    "            label = torch.tensor([label[0], x1, y1, x2, y2])\n",
    "            iou = box_iou(label[1:].to(\"cuda:0\").reshape(1, -1), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "            tp = validator.match_predictions(result.boxes.data[:,-1], label[0].to(\"cuda:0\").unsqueeze(0), iou)\n",
    "        \n",
    "            # update metric\n",
    "            tp = tp.detach().cpu().numpy()\n",
    "            conf = result.boxes.conf.detach().cpu().numpy()\n",
    "            pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "            target_cls = label[0].unsqueeze(0).detach().cpu().numpy()\n",
    "            \n",
    "        _, _, p, r, f1, f2, ap_50, ap_50_95, unique_classes, p_curve, r_curve, f1_curve, f2_curve, x, prec_values = ap_per_class(tp, conf, pre_cls, target_cls)\n",
    "        print(f2.mean(), unique_classes)\n",
    "        \n",
    "        # update df\n",
    "        df.loc[len(df)] = [result.path, p, r, f1, f2, ap_50, ap_50_95, conf, pre_cls, target_cls]\n",
    "\n",
    "    df.to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record/{source.split(\"/\")[-3]}_train.csv', index=False, header=True)\n",
    "    return df\n",
    "\n",
    "for i in os.listdir('/Data4/student_zhihan_data/data'):\n",
    "    # if i[-3:] != 'csv' and i != 'NEU-DET' and i != 'data.zip' and 'Gaussian' in i:\n",
    "    if i == 'GC10-DET':\n",
    "        source = os.path.join('/Data4/student_zhihan_data/data', i, 'train/images')\n",
    "        df = compute_f2(source, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: uncertainty * 3, proposed * score, NIQE, BRISQUE\n",
    "# output: quality score \n",
    "\n",
    "F2_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record'\n",
    "Proposed_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Score_Record'\n",
    "Uncertainty_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record'\n",
    "\n",
    "# concat all csv\n",
    "dfs = []\n",
    "for idx, dir in enumerate([F2_dir, Proposed_dir, Uncertainty_dir]):\n",
    "    files = os.listdir(dir)\n",
    "    for file in files:\n",
    "        if 'train' in file:\n",
    "            files.remove(file)\n",
    "\n",
    "    if idx == 0:\n",
    "        # df.to_csv(f'{dir}_new.csv', index=False, header=True)\n",
    "        df = None\n",
    "        for file in files:\n",
    "            if file[-7:] == 'new.csv':\n",
    "                if df is None:\n",
    "                    df = pd.read_csv(os.path.join(dir, file))\n",
    "                else:\n",
    "                    df = pd.concat([df, pd.read_csv(os.path.join(dir, file))], ignore_index=True)\n",
    "        df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record_new.csv')\n",
    "    elif idx == 1:\n",
    "        df = None\n",
    "        # df = pd.DataFrame(columns=['dataset','img_name','p', 'r', 'f1', 'f2', 'ap_50', 'ap_50_95', 'conf', 'pred_cls', 'target_cls'])\n",
    "        for file in files:\n",
    "            if file[-3:] == 'csv':\n",
    "                if df is None:\n",
    "                    df = pd.read_csv(os.path.join(dir, file))\n",
    "                else:\n",
    "                    df = pd.concat([df, pd.read_csv(os.path.join(dir, file))], ignore_index=True)\n",
    "        # df['img_name'] = df['dataset'] + '/images/' + df['img_name']\n",
    "        # df = df.drop(columns=['dataset'])\n",
    "        # df.to_csv(f'{dir}.csv', index=False, header=True)\n",
    "        df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record_new.csv')\n",
    "    else:\n",
    "        df = None\n",
    "        for file in files:\n",
    "            if file[-3:] == 'csv':\n",
    "                if df is None:\n",
    "                    df = pd.read_csv(os.path.join(dir, file))\n",
    "                else:\n",
    "                    df = pd.concat([df, pd.read_csv(os.path.join(dir, file))], ignore_index=True)\n",
    "        df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record_new.csv')\n",
    "        # df['img_name'] = df['dataset'] + '/test/images/' + df['img_name']\n",
    "        # df = df.drop(columns=['dataset'])\n",
    "        # df.to_csv(f'{dir}.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #combine\n",
    "# df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv')\n",
    "# df['img_name'] = df['dataset'] + '/test/images/' + df['img_name']\n",
    "# df = df.drop(columns=['dataset'])\n",
    "# df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv', index=False, header=True)\n",
    "\n",
    "# df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv')\n",
    "# df['img_name'] = df['dataset'] + '/images/' + df['img_name']\n",
    "# df = df.drop(columns=['dataset'])\n",
    "# df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2732998/2401473654.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, tmp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# files = os.listdir(Uncertainty_dir)\n",
    "# df = pd.DataFrame(columns=['dataset','img_name','objectness_uncertainty','objectness_entropy','weighted_variance_sum','weighted_entropy'])\n",
    "# for file in files:\n",
    "#     if file[-3:] == 'csv':\n",
    "#         tmp = pd.read_csv(os.path.join(Uncertainty_dir, file))\n",
    "#         #add header to tmp\n",
    "#         tmp.columns = ['dataset','img_name','objectness_uncertainty','objectness_entropy','weighted_variance_sum','weighted_entropy']\n",
    "#         df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "# df.to_csv(f'Uncertainty_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2732998/2492081121.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, tmp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# files = os.listdir(Proposed_dir)\n",
    "# df = pd.DataFrame(columns=['dataset', 'img_name', 'visibility', 'exposure'])\n",
    "# for file in files:\n",
    "#     if file[-3:] == 'csv':\n",
    "#         tmp = pd.read_csv(os.path.join(Proposed_dir, file))\n",
    "#         #add header to tmp\n",
    "#         tmp.columns = ['dataset', 'img_name', 'visibility', 'exposure']\n",
    "#         df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "# df.to_csv(f'Proposed_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: GC10-DET\n"
     ]
    }
   ],
   "source": [
    "# dfs = []\n",
    "# img_names =[]\n",
    "# for idx, dir in enumerate(['/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record_new.csv',\n",
    "#                           '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record_new.csv',\n",
    "#                            '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record_new.csv']):\n",
    "#     df = pd.read_csv(dir)\n",
    "#     df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "#     #cal the number of row with 'Gaussian' in img_name\n",
    "#     print(f'{idx}: {len(df[df[\"img_name\"].str.contains(\"GaussianBlur_13\")])}')\n",
    "#     img_names.append(df[df[\"img_name\"].str.contains(\"GaussianBlur_13\")]['img_name'])\n",
    "#     dfs.append(df)\n",
    " \n",
    "# merged_df = pd.merge(dfs[0], dfs[1], on=['img_name'])\n",
    "# merged_df = pd.merge(merged_df, dfs[2], on=['img_name'])\n",
    "# merged_df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Merged_final.csv')\n",
    "merged_df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/train_score/Merged_Training.csv')\n",
    "#set new column distortion equal to the first split of img_name column \n",
    "merged_df['distortion'] = merged_df['img_name'].apply(lambda x: x.split('/')[4])\n",
    "# merged_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "unique_distortion = merged_df['distortion'].unique()\n",
    "unique_distortion.sort()\n",
    "for idx, distortion in enumerate(unique_distortion):\n",
    "    print(f'{idx}: {distortion}')\n",
    "    merged_df.loc[merged_df['distortion'] == distortion, 'label'] = idx\n",
    "#set dtype of distortion column\n",
    "# merged_df['distortion'] = (merged_df['distortion']).astype(int)\n",
    "\n",
    "#get intersection of each list in img_names\n",
    "# intersection = set.intersection(set(img_names[0]), set(img_names[1]), set(img_names[2]))\n",
    "# print(f'intersection: {len(intersection)}')\n",
    "merged_df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/train_score/Merged_Training.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distortion\n",
       "GC10-DET    1593\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['distortion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "      <th>f1</th>\n",
       "      <th>ap_50</th>\n",
       "      <th>ap_50_95</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.237052</td>\n",
       "      <td>0.026693</td>\n",
       "      <td>0.213562</td>\n",
       "      <td>0.211834</td>\n",
       "      <td>1.875176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.627652</td>\n",
       "      <td>0.329721</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.165587</td>\n",
       "      <td>0.033949</td>\n",
       "      <td>0.404446</td>\n",
       "      <td>0.405391</td>\n",
       "      <td>0.611218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.448788</td>\n",
       "      <td>0.284848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.103034</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>1.141025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.206997</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>2.280131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.348250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.345798</td>\n",
       "      <td>0.037336</td>\n",
       "      <td>0.015718</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>2.285895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.584750</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.821447</td>\n",
       "      <td>0.197278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        visibility     exposure  objectness_uncertainty  \\\n",
       "count  1593.000000  1593.000000             1593.000000   \n",
       "mean      0.237052     0.026693                0.213562   \n",
       "std       0.165587     0.033949                0.404446   \n",
       "min       0.000231     0.000027                0.000000   \n",
       "25%       0.103034     0.003213                0.002766   \n",
       "50%       0.206997     0.011682                0.006289   \n",
       "75%       0.345798     0.037336                0.015718   \n",
       "max       0.821447     0.197278                1.000000   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy   f1        ap_50     ap_50_95  \\\n",
       "count            1593.000000       1593.000000  0.0  1593.000000  1593.000000   \n",
       "mean                0.211834          1.875176  NaN     0.627652     0.329721   \n",
       "std                 0.405391          0.611218  NaN     0.448788     0.284848   \n",
       "min                 0.000000          0.228020  NaN     0.000000     0.000000   \n",
       "25%                 0.000255          1.141025  NaN     0.000000     0.000000   \n",
       "50%                 0.001140          2.280131  NaN     0.995000     0.348250   \n",
       "75%                 0.016527          2.285895  NaN     0.995000     0.584750   \n",
       "max                 1.000000          2.302585  NaN     0.995000     0.995000   \n",
       "\n",
       "        label  \n",
       "count  1593.0  \n",
       "mean      0.0  \n",
       "std       0.0  \n",
       "min       0.0  \n",
       "25%       0.0  \n",
       "50%       0.0  \n",
       "75%       0.0  \n",
       "max       0.0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "      <th>ap_50</th>\n",
       "      <th>ap_50_95</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.132500e+04</td>\n",
       "      <td>11450.000000</td>\n",
       "      <td>11221.000000</td>\n",
       "      <td>11221.000000</td>\n",
       "      <td>11221.000000</td>\n",
       "      <td>11326.000000</td>\n",
       "      <td>11326.000000</td>\n",
       "      <td>11450.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.651742e-01</td>\n",
       "      <td>0.158247</td>\n",
       "      <td>0.418748</td>\n",
       "      <td>0.418508</td>\n",
       "      <td>2.034734</td>\n",
       "      <td>0.389393</td>\n",
       "      <td>0.195023</td>\n",
       "      <td>24.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.006516e-01</td>\n",
       "      <td>0.248522</td>\n",
       "      <td>0.489448</td>\n",
       "      <td>0.489724</td>\n",
       "      <td>0.516566</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.264879</td>\n",
       "      <td>14.4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.220000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.058247e-01</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>2.271354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.264281e-01</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.013464</td>\n",
       "      <td>2.284308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.793976e-01</td>\n",
       "      <td>0.203864</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>37.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.889542e-01</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>49.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         visibility      exposure  objectness_uncertainty  \\\n",
       "count  1.132500e+04  11450.000000            11221.000000   \n",
       "mean   2.651742e-01      0.158247                0.418748   \n",
       "std    2.006516e-01      0.248522                0.489448   \n",
       "min    3.220000e-07      0.000000                0.000000   \n",
       "25%    1.058247e-01      0.000897                0.005022   \n",
       "50%    2.264281e-01      0.029070                0.012798   \n",
       "75%    3.793976e-01      0.203864                1.000000   \n",
       "max    9.889542e-01      0.999990                1.000000   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy         ap_50      ap_50_95  \\\n",
       "count           11221.000000      11221.000000  11326.000000  11326.000000   \n",
       "mean                0.418508          2.034734      0.389393      0.195023   \n",
       "std                 0.489724          0.516566      0.460851      0.264879   \n",
       "min                 0.000000          0.325677      0.000000      0.000000   \n",
       "25%                 0.000811          2.271354      0.000000      0.000000   \n",
       "50%                 0.013464          2.284308      0.000000      0.000000   \n",
       "75%                 1.000000          2.302585      0.995000      0.398000   \n",
       "max                 1.000000          2.302585      0.995000      0.995000   \n",
       "\n",
       "            label  \n",
       "count  11450.0000  \n",
       "mean      24.5000  \n",
       "std       14.4315  \n",
       "min        0.0000  \n",
       "25%       12.0000  \n",
       "50%       24.5000  \n",
       "75%       37.0000  \n",
       "max       49.0000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_df['weighted_entropy'] = merged_df['weighted_variance_sum']\n",
    "# merged_df['weighted_variance_sum'] = merged_df['objectness_entropy']\n",
    "# merged_df.drop(columns=['objectness_entropy'], inplace=True)\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged_new.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 [1 9]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "1.0 [8]\n",
      "0.8125 [1 9]\n",
      "1.0 [6]\n",
      "0.0 [0]\n",
      "0.0 [8]\n",
      "1.0 [7]\n",
      "0.0 [7]\n",
      "1.0 [1 9]\n",
      "0.8333333333333334 [1]\n",
      "0.0 [2]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [5]\n",
      "0.8861831775579442 [3]\n",
      "0.0 [8]\n",
      "0.5 [3]\n",
      "0.9894106530883233 [1 9]\n",
      "1.0 [1 9]\n",
      "0.0 [3]\n",
      "1.0 [4]\n",
      "0.0 [9]\n",
      "0.25 [1 4 8 9]\n",
      "1.0 [6]\n",
      "0.955061678002223 [4 9]\n",
      "1.0 [8]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "1.0 [6 8]\n",
      "1.0 [8]\n",
      "0.0 [6]\n",
      "1.0 [4]\n",
      "0.9698366958523534 [9]\n",
      "0.8333333333333334 [9]\n",
      "1.0 [8]\n",
      "0.9166666666666667 [4 9]\n",
      "1.0 [7]\n",
      "1.0 [6]\n",
      "0.9784231710625808 [6]\n",
      "0.0 [6]\n",
      "1.0 [4]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [2]\n",
      "1.0 [4 9]\n",
      "0.969202609172539 [4 9]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "0.6682954202760385 [3]\n",
      "0.0 [6]\n",
      "0.9784407998647183 [7]\n",
      "0.8620689655172415 [3]\n",
      "1.0 [1 9]\n",
      "1.0 [4 9]\n",
      "0.0 [2]\n",
      "1.0 [7]\n",
      "0.0 [0]\n",
      "0.5 [1 9]\n",
      "0.9166666666666667 [6 8]\n",
      "1.0 [8]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [7]\n",
      "0.9704364934628894 [8]\n",
      "0.0 [2]\n",
      "0.0 [5]\n",
      "0.5 [3]\n",
      "0.0 [6]\n",
      "0.0 [2 6]\n",
      "0.0 [2]\n",
      "0.0 [6]\n",
      "1.0 [1]\n",
      "1.0 [3]\n",
      "1.0 [6]\n",
      "0.0 [6 8]\n",
      "0.0 [6]\n",
      "0.0 [5]\n",
      "0.0 [7]\n",
      "0.8282790301542053 [3]\n",
      "1.0 [9]\n",
      "0.0 [2]\n",
      "0.9166666666666667 [1 9]\n",
      "1.0 [8]\n",
      "1.0 [1 9]\n",
      "1.0 [6]\n",
      "0.0 [3]\n",
      "0.0 [2 8]\n",
      "1.0 [9]\n",
      "1.0 [8]\n",
      "0.0 [9]\n",
      "1.0 [9]\n",
      "0.0 [5]\n",
      "0.969870198914854 [1]\n",
      "1.0 [4]\n",
      "1.0 [8]\n",
      "0.0 [6]\n",
      "0.9166666666666667 [4 9]\n",
      "0.0 [8]\n",
      "0.0 [2 6]\n",
      "0.0 [3]\n",
      "1.0 [1]\n",
      "0.5 [6 9]\n",
      "0.0 [6]\n",
      "0.0 [8]\n",
      "1.0 [6]\n",
      "0.3333333333333333 [4 5 9]\n",
      "1.0 [3]\n",
      "1.0 [6]\n",
      "0.8333333333333334 [7]\n",
      "1.0 [9]\n",
      "1.0 [1 9]\n",
      "0.0 [7]\n",
      "1.0 [6]\n",
      "0.0 [2]\n",
      "0.8333333333333334 [8]\n",
      "0.48425917805839425 [0 3 4 9]\n",
      "0.0 [6]\n",
      "0.9664022636197249 [1 9]\n",
      "0.625 [6]\n",
      "0.0 [2 8]\n",
      "0.0 [8]\n",
      "1.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6 8]\n",
      "1.0 [8]\n",
      "0.0 [6]\n",
      "1.0 [6]\n",
      "0.5555555555555556 [6]\n",
      "0.5 [4 6]\n",
      "1.0 [1]\n",
      "0.9383478324058097 [3]\n",
      "0.2777777777777778 [3]\n",
      "0.915059415479969 [6]\n",
      "1.0 [7]\n",
      "0.966537216274089 [1]\n",
      "1.0 [4 9]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "1.0 [8]\n",
      "1.0 [9]\n",
      "0.8333333333333334 [6]\n",
      "1.0 [6]\n",
      "0.8333333333333334 [6]\n",
      "0.9538502012242512 [6]\n",
      "0.0 [6]\n",
      "1.0 [7]\n",
      "0.0 [6]\n",
      "0.0 [2]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.5555555555555556 [6]\n",
      "0.9761161289246528 [6]\n",
      "0.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [8]\n",
      "1.0 [4 9]\n",
      "1.0 [1]\n",
      "1.0 [1 9]\n",
      "1.0 [7]\n",
      "1.0 [6]\n",
      "1.0 [9]\n",
      "0.6666666666666666 [4 6 9]\n",
      "1.0 [9]\n",
      "1.0 [4]\n",
      "0.9817342520366794 [3]\n",
      "1.0 [6]\n",
      "0.0 [3 6]\n",
      "1.0 [6]\n",
      "0.0 [8]\n",
      "1.0 [4 9]\n",
      "1.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.8333333333333334 [6]\n",
      "1.0 [4 9]\n",
      "0.0 [3]\n",
      "0.5 [4 9]\n",
      "0.3333333333333333 [2]\n",
      "0.3333333333333333 [1 3 9]\n",
      "0.0 [2]\n",
      "1.0 [9]\n",
      "1.0 [1]\n",
      "0.5555555555555556 [3]\n",
      "1.0 [1]\n",
      "0.0 [8]\n",
      "0.4166666666666667 [6 8]\n",
      "1.0 [1]\n",
      "0.0 [7]\n",
      "0.0 [4]\n",
      "1.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [2]\n",
      "1.0 [1]\n",
      "1.0 [9]\n",
      "1.0 [8]\n",
      "0.9738349911183414 [1 9]\n",
      "0.5 [4 8]\n",
      "0.9674976704199053 [9]\n",
      "0.0 [5]\n",
      "0.0 [7]\n",
      "0.9583904309214064 [1]\n",
      "0.3846153846153846 [3]\n",
      "1.0 [4 9]\n",
      "0.0 [2]\n",
      "0.0 [0]\n",
      "1.0 [4]\n",
      "0.0 [9]\n",
      "0.5 [6 8]\n",
      "1.0 [1 9]\n",
      "0.9606150748563909 [1 9]\n",
      "0.9107177457645993 [8]\n",
      "0.0 [3]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.5555555555555556 [8]\n",
      "1.0 [8]\n",
      "1.0 [4 9]\n",
      "0.5555555555555556 [6]\n",
      "0.8333333333333334 [6]\n",
      "0.9796323665356054 [6]\n",
      "0.0 [6]\n",
      "0.7952832871373511 [6]\n",
      "1.0 [8]\n",
      "1.0 [2]\n",
      "0.0 [9]\n",
      "1.0 [7]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.engine.validator import BaseValidator\n",
    "from ultralytics.utils.metrics import box_iou, Metric, DetMetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# load txt label\n",
    "\n",
    "\n",
    "validator = BaseValidator()\n",
    "validator.iouv = torch.arange(0.5, 1, 0.05)\n",
    "\n",
    "for result in results:\n",
    "    metric = DetMetrics()\n",
    "    label_path = result.path.replace('images', 'labels')[:-4] + '.txt'\n",
    "    label = torch.from_numpy(np.loadtxt(label_path))\n",
    "    # turn label into (x1, y1, x2, y2) format\n",
    "    \n",
    "    if label.dim() > 1:\n",
    "        x1 = label[:, 1] - label[:, 3] / 2\n",
    "        y1 = label[:, 2] - label[:, 4] / 2\n",
    "        x2 = label[:, 1] + label[:, 3] / 2\n",
    "        y2 = label[:, 2] + label[:, 4] / 2\n",
    "        label = torch.stack((label[:, 0], x1, y1, x2, y2), 1)\n",
    "        iou = box_iou(label[:, 1:].to(\"cuda:0\"), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "        tp = validator.match_predictions(result.boxes.data[:,-1], label[:, 0].to(\"cuda:0\"), iou)\n",
    "        \n",
    "        # update metric\n",
    "        tp = tp.detach().cpu().numpy()\n",
    "        conf = result.boxes.conf.detach().cpu().numpy()\n",
    "        pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "        target_cls = label[:, 0].detach().cpu().numpy()\n",
    "    \n",
    "    else:\n",
    "        x1 = label[1] - label[3] / 2\n",
    "        y1 = label[2] - label[4] / 2\n",
    "        x2 = label[1] + label[3] / 2\n",
    "        y2 = label[2] + label[4] / 2\n",
    "        label = torch.tensor([label[0], x1, y1, x2, y2])\n",
    "        iou = box_iou(label[1:].to(\"cuda:0\").reshape(1, -1), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "        tp = validator.match_predictions(result.boxes.data[:,-1], label[0].to(\"cuda:0\").unsqueeze(0), iou)\n",
    "    \n",
    "        # update metric\n",
    "        tp = tp.detach().cpu().numpy()\n",
    "        conf = result.boxes.conf.detach().cpu().numpy()\n",
    "        pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "        target_cls = label[0].unsqueeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    _, _, p, r, f1, f2, ap50, ap_50_95, unique_classes, p_curve, r_curve, f1_curve, f2_curve, x, prec_values = ap_per_class(tp, conf, pre_cls, target_cls)\n",
    "    print(f2.mean(), unique_classes)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results generator\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bbox outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11356/2759775949.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['objectness_uncertainty'] = X_test['objectness_uncertainty'].fillna(1)\n",
      "/tmp/ipykernel_11356/2759775949.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['weighted_variance_sum'] = X_test['weighted_variance_sum'].fillna(1)\n",
      "/tmp/ipykernel_11356/2759775949.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['weighted_entropy'] = X_test['weighted_entropy'].fillna(np.log(10))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def safe_convert_to_list(s):\n",
    "    try:\n",
    "        # Attempt to directly evaluate the string\n",
    "        return ast.literal_eval(s)\n",
    "    except SyntaxError:\n",
    "        # If direct evaluation fails, attempt to manually parse the string\n",
    "        cleaned_str = s.strip('[]')\n",
    "        if cleaned_str:  # Check if the string is not empty\n",
    "            numbers = [float(num) for num in cleaned_str.split() if num not in ['[', ']']]\n",
    "            return numbers\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "# Load the dataset\n",
    "test_path = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Merged_final.csv'\n",
    "train_path = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/train_score/Merged_Training.csv'\n",
    "for data_path in [test_path, train_path, ]:\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    # Display the first few rows of the dataframe to understand its structure\n",
    "    data.head()\n",
    "\n",
    "\n",
    "    # Apply the conversion function to the 'f2' column and then compute the mean F2 score\n",
    "    data['f2'] = data['f2'].fillna('[]')\n",
    "    data['f2'] = data['f2'].apply(safe_convert_to_list)\n",
    "    data['mean_f2'] = data['f2'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "\n",
    "\n",
    "    # data = data[data['img_name'].str.contains('/GC10-DET/')]\n",
    "\n",
    "    # Prepare the dataset for modeling\n",
    "    # features = ['visibility', 'exposure', 'objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy', 'label']\n",
    "    # features = ['visibility', 'exposure', 'label']\n",
    "    features = ['objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy', 'label']\n",
    "\n",
    "    # features = ['visibility', 'exposure']\n",
    "    # features = ['objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy']\n",
    "    if data_path == train_path:\n",
    "        X_train = data[features]\n",
    "        y_train = data['mean_f2']\n",
    "    else:\n",
    "        tmp = data\n",
    "        X_test = data[features]\n",
    "        y_test = data['mean_f2']\n",
    "        # fill nan\n",
    "        # X_test['visibility'] = X_test['visibility'].fillna(0)\n",
    "        X_test['objectness_uncertainty'] = X_test['objectness_uncertainty'].fillna(1)\n",
    "        X_test['weighted_variance_sum'] = X_test['weighted_variance_sum'].fillna(1)\n",
    "        X_test['weighted_entropy'] = X_test['weighted_entropy'].fillna(np.log(10))\n",
    "        \n",
    "    # y = data['ap_50_95']\n",
    "    # y = data['ap_50']\n",
    "\n",
    "# standard scale X\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# Display the first few rows of features and target to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11450.000000</td>\n",
       "      <td>11450.000000</td>\n",
       "      <td>11450.000000</td>\n",
       "      <td>11450.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.430373</td>\n",
       "      <td>0.430138</td>\n",
       "      <td>2.040091</td>\n",
       "      <td>24.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.491315</td>\n",
       "      <td>0.491590</td>\n",
       "      <td>0.512747</td>\n",
       "      <td>14.4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325677</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>2.272682</td>\n",
       "      <td>12.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.013622</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>2.284593</td>\n",
       "      <td>24.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>37.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>49.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       objectness_uncertainty  weighted_variance_sum  weighted_entropy  \\\n",
       "count            11450.000000           11450.000000      11450.000000   \n",
       "mean                 0.430373               0.430138          2.040091   \n",
       "std                  0.491315               0.491590          0.512747   \n",
       "min                  0.000000               0.000000          0.325677   \n",
       "25%                  0.005143               0.000848          2.272682   \n",
       "50%                  0.013622               0.015310          2.284593   \n",
       "75%                  1.000000               1.000000          2.302585   \n",
       "max                  1.000000               1.000000          2.302585   \n",
       "\n",
       "            label  \n",
       "count  11450.0000  \n",
       "mean      24.5000  \n",
       "std       14.4315  \n",
       "min        0.0000  \n",
       "25%       12.0000  \n",
       "50%       24.5000  \n",
       "75%       37.0000  \n",
       "max       49.0000  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().any()\n",
    "X_test.isna().any()\n",
    "# y_test.isna().any()\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.000000</td>\n",
       "      <td>1593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.213562</td>\n",
       "      <td>0.211834</td>\n",
       "      <td>1.875176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.404446</td>\n",
       "      <td>0.405391</td>\n",
       "      <td>0.611218</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>1.141025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>2.280131</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.015718</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>2.285895</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       objectness_uncertainty  weighted_variance_sum  weighted_entropy   label\n",
       "count             1593.000000            1593.000000       1593.000000  1593.0\n",
       "mean                 0.213562               0.211834          1.875176     0.0\n",
       "std                  0.404446               0.405391          0.611218     0.0\n",
       "min                  0.000000               0.000000          0.228020     0.0\n",
       "25%                  0.002766               0.000255          1.141025     0.0\n",
       "50%                  0.006289               0.001140          2.280131     0.0\n",
       "75%                  0.015718               0.016527          2.285895     0.0\n",
       "max                  1.000000               1.000000          2.302585     0.0"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6648476871601723 [      10.58     -11.226    0.039008]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>SRCC</th>\n",
       "      <th>PLCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.142849</td>\n",
       "      <td>0.314198</td>\n",
       "      <td>0.589480</td>\n",
       "      <td>0.603036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Regressor</th>\n",
       "      <td>0.204128</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.414991</td>\n",
       "      <td>0.491255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP Regressor</th>\n",
       "      <td>0.151094</td>\n",
       "      <td>0.274612</td>\n",
       "      <td>0.568346</td>\n",
       "      <td>0.582809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regressor</th>\n",
       "      <td>0.142922</td>\n",
       "      <td>0.313848</td>\n",
       "      <td>0.611164</td>\n",
       "      <td>0.605066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MSE        R2      SRCC      PLCC\n",
       "Linear Regression        0.142849  0.314198  0.589480  0.603036\n",
       "Decision Tree Regressor  0.204128  0.020002  0.414991  0.491255\n",
       "MLP Regressor            0.151094  0.274612  0.568346  0.582809\n",
       "Random Forest Regressor  0.142922  0.313848  0.611164  0.605066"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "SRCC = []\n",
    "PLCC = []\n",
    "MSE = []\n",
    "R2 = []\n",
    "\n",
    "# kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# for train_index, test_index in kf.split(data.index):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     #add noise to X_train\n",
    "#     X_train = X_train + np.random.normal(0, 0.01, X_train.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train = X_train[:-1] + np.random.normal(0, 0.0001, X_train[:-1].shape)\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "# X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"MLP Regressor\": MLPRegressor(random_state=42, max_iter=1000), # Increased max_iter for convergence\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "features.remove('label')\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train.loc[:,features], y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test.loc[:,features])\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    srcc = stats.spearmanr(y_test, y_pred)[0]\n",
    "    plcc = stats.pearsonr(y_test, y_pred)[0]\n",
    "    \n",
    "    if name == \"Random Forest Regressor\":                       \n",
    "        SRCC.append(srcc)\n",
    "        PLCC.append(plcc)\n",
    "        MSE.append(mse)\n",
    "        R2.append(r2)\n",
    "    \n",
    "    results[name] = {\"MSE\": mse, \"R2\": r2, \"SRCC\": srcc, \"PLCC\": plcc}\n",
    "    \n",
    "    if name == 'Decision Tree Regressor':\n",
    "        # save test resulst to csv\n",
    "        df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "        df.to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/test_result.csv', index=False)\n",
    "    \n",
    "    if name == 'Linear Regression':\n",
    "        print(model.intercept_, model.coef_)\n",
    "        \n",
    "    #print_result\n",
    "\n",
    "results_df = pd.DataFrame(results).T  # Convert results to a DataFrame for better readability\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.20476,     0.56638,     0.22886])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24698101096602526 -0.1857307058313724 0.02512071480735943 0.07130610239294317\n"
     ]
    }
   ],
   "source": [
    "combine = 1/2 * X_test['visibility'] + 1/2 * (1 - X_test['exposure'])\n",
    "mse = mean_squared_error(y_test, combine)\n",
    "r2 = r2_score(y_test, combine)\n",
    "srcc = stats.spearmanr(y_test, combine)[0]\n",
    "plcc = stats.pearsonr(y_test, combine)[0]\n",
    "print(mse, r2, srcc, plcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = X_test['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GC10-DET'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[tmp['label'] == 0]['distortion'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11356/1607485480.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['predict'] = y_pred\n",
      "/tmp/ipykernel_11356/1607485480.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['predict'] = model.predict(X_train.loc[:,features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC10-DET_brightness_-15 229\n",
      "GC10-DET_Transform_Scale_0.0:0.05 229\n",
      "GC10-DET_Transform_Scale_0.15:0.2 229\n",
      "GC10-DET_MedianBlur_29 229\n",
      "GC10-DET_brightness_50 229\n",
      "GC10-DET_brightness_110 229\n",
      "GC10-DET_brightness_-150 229\n",
      "GC10-DET_GaussianBlur_3 229\n",
      "GC10-DET_brightness_-20 229\n",
      "GC10-DET_Transform_Scale_0.05:0.1 229\n",
      "GC10-DET_GaussianBlur_7 229\n",
      "GC10-DET_BilateralBlur_300 229\n",
      "GC10-DET_Transform_Scale_0.1:0.15 229\n",
      "GC10-DET_Sharpen_5 229\n",
      "GC10-DET_BilateralBlur_60 229\n",
      "GC10-DET_brightness_70 229\n",
      "GC10-DET_GaussianBlur_11 229\n",
      "GC10-DET_GaussianBlur_17 229\n",
      "GC10-DET_Sharpening_1.5 229\n",
      "GC10-DET_BilateralBlur_120 229\n",
      "GC10-DET_Transform_Scale_0.25:0.3 229\n",
      "GC10-DET_Sharpen_10 229\n",
      "GC10-DET_GaussianBlur_15 229\n",
      "GC10-DET_Sharpening_2.0 229\n",
      "GC10-DET_MedianBlur_43 229\n",
      "GC10-DET_Sharpening_3 229\n",
      "GC10-DET_MedianBlur_71 229\n",
      "GC10-DET 229\n",
      "GC10-DET_GaussianBlur_9 229\n",
      "GC10-DET_MedianBlur_57 229\n",
      "GC10-DET_BilateralBlur_240 229\n",
      "GC10-DET_brightness_30 229\n",
      "GC10-DET_MedianBlur_15 229\n",
      "GC10-DET_GaussianBlur_5 229\n",
      "GC10-DET_brightness_-30 229\n",
      "GC10-DET_Transform_Scale_0.2:0.25 229\n",
      "GC10-DET_brightness_-10 229\n",
      "GC10-DET_Sharpening_2.5 229\n",
      "GC10-DET_brightness_10 229\n",
      "GC10-DET_Sharpen_15 229\n",
      "GC10-DET_brightness_90 229\n",
      "GC10-DET_Sharpen_20 229\n",
      "GC10-DET_BilateralBlur_180 229\n",
      "GC10-DET_Sharpen_25 229\n",
      "GC10-DET_brightness_20 229\n",
      "GC10-DET_brightness_-100 229\n",
      "GC10-DET_brightness_60 229\n",
      "GC10-DET_brightness_-50 229\n",
      "GC10-DET_GaussianBlur_13 229\n",
      "GC10-DET_Sharpening_0.5 229\n"
     ]
    }
   ],
   "source": [
    "X_test['predict'] = y_pred\n",
    "X_train['predict'] = model.predict(X_train.loc[:,features])\n",
    "# X_combine = pd.concat([X_train, X_test]) \n",
    "# y_combine = pd.concat([y_train, y_test])\n",
    "\n",
    "results = pd.DataFrame(columns=['dataset', 'predict', 'number'])\n",
    "# X_test.to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/random_forest_result.csv', index=False)\n",
    "for label in dirs:\n",
    "    #cal average predict group by label\n",
    "    results.loc[len(results)] = [tmp[tmp['label'] == label]['distortion'].iloc[0], X_test.loc[X_test[\"label\"] == label, \"predict\"].mean(), (X_test['label'] == label).sum()]\n",
    "    print(tmp[tmp['label'] == label]['distortion'].iloc[0], (X_test['label'] == label).sum())\n",
    "# results.to_excel(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/regression_result.xlsx', index=False) \n",
    "\n",
    "# Excel\n",
    "file_path = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/regression_result.xlsx'\n",
    "\n",
    "# ExcelWritermode'a'\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "    # DataFramesheet'NewSheet'\n",
    "    results.to_excel(writer, sheet_name='Uncertainty')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'describe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'describe'"
     ]
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "      <th>SRCC</th>\n",
    "      <th>PLCC</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.193364</td>\n",
    "      <td>0.095085</td>\n",
    "      <td>0.307203</td>\n",
    "      <td>0.309837</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.271613</td>\n",
    "      <td>-0.271107</td>\n",
    "      <td>0.370684</td>\n",
    "      <td>0.366573</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.164739</td>\n",
    "      <td>0.229049</td>\n",
    "      <td>0.490648</td>\n",
    "      <td>0.482423</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.148216</td>\n",
    "      <td>0.306374</td>\n",
    "      <td>0.550785</td>\n",
    "      <td>0.554713</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "      <th>SRCC</th>\n",
    "      <th>PLCC</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.065377</td>\n",
    "      <td>0.131179</td>\n",
    "      <td>0.348499</td>\n",
    "      <td>0.364317</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.096486</td>\n",
    "      <td>-0.282236</td>\n",
    "      <td>0.385156</td>\n",
    "      <td>0.375813</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.068393</td>\n",
    "      <td>0.091110</td>\n",
    "      <td>0.340770</td>\n",
    "      <td>0.308201</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.047503</td>\n",
    "      <td>0.368724</td>\n",
    "      <td>0.598867</td>\n",
    "      <td>0.609294</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "      <th>SRCC</th>\n",
    "      <th>PLCC</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.193365</td>\n",
    "      <td>0.095085</td>\n",
    "      <td>0.307206</td>\n",
    "      <td>0.309837</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.272660</td>\n",
    "      <td>-0.276006</td>\n",
    "      <td>0.349884</td>\n",
    "      <td>0.361031</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.190556</td>\n",
    "      <td>0.108228</td>\n",
    "      <td>0.324465</td>\n",
    "      <td>0.333133</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.147864</td>\n",
    "      <td>0.308021</td>\n",
    "      <td>0.550822</td>\n",
    "      <td>0.556283</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.197254</td>\n",
    "      <td>0.064327</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.380059</td>\n",
    "      <td>-0.802814</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.193648</td>\n",
    "      <td>0.081430</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.213226</td>\n",
    "      <td>-0.011437</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.199501</td>\n",
    "      <td>0.053668</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.283149</td>\n",
    "      <td>-0.343119</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.200023</td>\n",
    "      <td>0.051190</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.161334</td>\n",
    "      <td>0.234711</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.188070</td>\n",
    "      <td>0.107889</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.275943</td>\n",
    "      <td>-0.308935</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.185389</td>\n",
    "      <td>0.120607</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.140655</td>\n",
    "      <td>0.332804</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.16596,     0.16575,     0.17716,     0.26334,     0.22779])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1.217000e+03</td>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1217.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.270493</td>\n",
       "      <td>0.230253</td>\n",
       "      <td>6.240577e-03</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.001661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.209810</td>\n",
       "      <td>0.272485</td>\n",
       "      <td>4.841622e-03</td>\n",
       "      <td>0.073080</td>\n",
       "      <td>0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.902979e-08</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.093177</td>\n",
       "      <td>0.010945</td>\n",
       "      <td>2.279311e-03</td>\n",
       "      <td>0.097146</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.230777</td>\n",
       "      <td>0.109753</td>\n",
       "      <td>5.438593e-03</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.402377</td>\n",
       "      <td>0.393562</td>\n",
       "      <td>8.849673e-03</td>\n",
       "      <td>0.226471</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.958951</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>2.272954e-02</td>\n",
       "      <td>0.413786</td>\n",
       "      <td>0.040970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        visibility     exposure  objectness_uncertainty  \\\n",
       "count  1217.000000  1217.000000            1.217000e+03   \n",
       "mean      0.270493     0.230253            6.240577e-03   \n",
       "std       0.209810     0.272485            4.841622e-03   \n",
       "min       0.000420     0.000000            3.902979e-08   \n",
       "25%       0.093177     0.010945            2.279311e-03   \n",
       "50%       0.230777     0.109753            5.438593e-03   \n",
       "75%       0.402377     0.393562            8.849673e-03   \n",
       "max       0.958951     0.999026            2.272954e-02   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy  \n",
       "count            1217.000000       1217.000000  \n",
       "mean                0.149462          0.001661  \n",
       "std                 0.073080          0.004813  \n",
       "min                 0.055160          0.000000  \n",
       "25%                 0.097146          0.000037  \n",
       "50%                 0.107024          0.000173  \n",
       "75%                 0.226471          0.000954  \n",
       "max                 0.413786          0.040970  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5651.000000\n",
       "mean        0.240280\n",
       "std         0.275259\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.099500\n",
       "75%         0.466655\n",
       "max         0.995000\n",
       "Name: ap_50_95, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1413.000000\n",
       "mean        0.471483\n",
       "std         0.462422\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: mean_f2, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyiqa.archs.niqe_arch import *\n",
    "from pyiqa.utils import load_file_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model_urls = {\n",
    "    'url': 'https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/niqe_modelparameters.mat',\n",
    "    'niqe': 'https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/niqe_modelparameters.mat',\n",
    "    'ilniqe': 'https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/ILNIQE_templateModel.mat',\n",
    "    'pretrained': '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1259.6615, dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "path = '/Data4/student_zhihan_data/data/GC10-DET_brightness_110/test/images/img_01_425000300_00630_jpg.rf.12001adc8b86faf88a47b6aa6f321b91.jpg'\n",
    "img = cv2.imread(path)\n",
    "img = torch.from_numpy(img)\n",
    "img = img.permute(2,0,1).unsqueeze(0)\n",
    "# calculate_niqe(img, color_space='gray', pretrained_model_path=load_file_from_url(default_model_urls['niqe']))\n",
    "# calculate_ilniqe(img, color_space='gray', pretrained_model_path=default_model_urls['pretrained'])\n",
    "calculate_niqe(img, color_space='gray', pretrained_model_path='model.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matlab model\n",
    "from scipy.io import loadmat\n",
    "covar = loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/covar.mat')\n",
    "mean = loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/mean.mat')\n",
    "model = loadmat(load_file_from_url(default_model_urls['niqe']))\n",
    "model['mu_prisparam'] = np.array(mean['mean'])\n",
    "model['cov_prisparam'] = np.array(covar['covariance'])\n",
    "# save mat model\n",
    "import scipy.io\n",
    "scipy.io.savemat('model.mat', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat')\n",
    "# change model key name 'None' to 'templateModel'\n",
    "model['templateModel'] = model.pop('None')\n",
    "scipy.io.savemat('model.mat', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Feb 14 16:14:01 2024',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'mu_prisparam': array([[     2.8918,      1.1237,     0.94071,  -0.0074117,     0.30461,     0.29457,     0.92068,     0.16218,     0.21715,     0.36395,     0.93394,   -0.093931,     0.33724,     0.25074,       0.934,   -0.094519,     0.33749,     0.25046,      2.7161,       1.036,     0.91893,    0.018017,     0.24276,\n",
       "             0.25803,     0.90293,     0.12705,     0.18806,     0.29925,     0.89492,   -0.053007,     0.26442,     0.21893,     0.89485,   -0.055262,     0.26535,     0.21794]]),\n",
       " 'cov_prisparam': array([[    0.11128,     0.04245,    0.031441, ...,   -0.003672,    0.022724,    0.018208],\n",
       "        [    0.04245,    0.022494,    0.013837, ...,  -0.0016413,    0.010588,   0.0086856],\n",
       "        [   0.031441,    0.013837,   0.0098078, ...,  -0.0011065,   0.0070665,   0.0057277],\n",
       "        ...,\n",
       "        [  -0.003672,  -0.0016413,  -0.0011065, ...,  0.00049509,  -0.0010687, -0.00059319],\n",
       "        [   0.022724,    0.010588,   0.0070665, ...,  -0.0010687,   0.0068859,   0.0055151],\n",
       "        [   0.018208,   0.0086856,   0.0057277, ..., -0.00059319,   0.0055151,   0.0046443]])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri Aug 24 17:52:00 2012',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'mu_prisparam': array([[     2.6013,      0.9057,     0.81205,    0.090427,     0.13873,     0.20603,     0.81897,    0.062462,     0.15333,     0.19591,     0.82647,   -0.025526,     0.18857,     0.16578,     0.82429,   -0.025361,     0.18724,     0.16505,      2.9695,     0.96123,     0.84935,    0.082383,     0.16132,\n",
       "             0.22492,     0.85895,    0.055084,     0.17531,     0.21713,     0.87208,   -0.032221,     0.21549,     0.18821,      0.8694,   -0.032326,     0.21474,     0.18678]]),\n",
       " 'cov_prisparam': array([[    0.45348,    0.096101,    0.082763, ...,  -0.0068539,    0.041395,    0.031916],\n",
       "        [   0.096101,    0.037112,    0.021553, ...,  -0.0032338,    0.012877,   0.0095948],\n",
       "        [   0.082763,    0.021553,    0.017707, ...,  -0.0016373,   0.0089932,   0.0069435],\n",
       "        ...,\n",
       "        [ -0.0068539,  -0.0032338,  -0.0016373, ...,    0.002787,  -0.0028321, -0.00035564],\n",
       "        [   0.041395,    0.012877,   0.0089932, ...,  -0.0028321,   0.0093344,   0.0061423],\n",
       "        [   0.031916,   0.0095948,   0.0069435, ..., -0.00035564,   0.0061423,   0.0054225]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadmat(load_file_from_url(default_model_urls['niqe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/pyiqa/matlab_utils/functions.py:167: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at ../aten/src/ATen/native/Correlation.cpp:117.)\n",
      "  return torch.cov(tensor, correction=correction)\n"
     ]
    }
   ],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "import cv2\n",
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged_new.csv')\n",
    "# calculate niqe for each row and add to new column\n",
    "for idx, row in df.iterrows():\n",
    "    img = cv2.imread(row['img_name'])\n",
    "    img = torch.from_numpy(img)\n",
    "    img = img.permute(2,0,1).unsqueeze(0)\n",
    "    try:\n",
    "        niqe = calculate_niqe(img, color_space='gray', pretrained_model_path='/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat')\n",
    "        df.loc[idx, 'niqe'] = niqe.item()\n",
    "    except:\n",
    "        #set NAN\n",
    "        niqe = np.nan\n",
    "        df.loc[idx, 'niqe'] = niqe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged_new.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2718285/2985616201.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['niqe'] = data['niqe'] / data['niqe'].max()\n"
     ]
    }
   ],
   "source": [
    "data = df.dropna()\n",
    "\n",
    "# scale niqe to [0,1]\n",
    "# data['niqe'] = (data['niqe'] - data['niqe'].min()) / (data['niqe'].max() - data['niqe'].min())\n",
    "data['niqe'] = data['niqe'] / data['niqe'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2718285/729834478.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['f2'] = data['f2'].apply(safe_convert_to_list)\n",
      "/tmp/ipykernel_2718285/729834478.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['mean_f2'] = data['f2'].apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "# Apply the conversion function to the 'f2' column and then compute the mean F2 score\n",
    "def safe_convert_to_list(s):\n",
    "    try:\n",
    "        # Attempt to directly evaluate the string\n",
    "        return ast.literal_eval(s)\n",
    "    except SyntaxError:\n",
    "        # If direct evaluation fails, attempt to manually parse the string\n",
    "        cleaned_str = s.strip('[]')\n",
    "        if cleaned_str:  # Check if the string is not empty\n",
    "            numbers = [float(num) for num in cleaned_str.split() if num not in ['[', ']']]\n",
    "            return numbers\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "data['f2'] = data['f2'].apply(safe_convert_to_list)\n",
    "data['mean_f2'] = data['f2'].apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "\n",
    "# Prepare the dataset for modeling\n",
    "features = ['visibility', 'exposure', 'objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy', 'niqe']\n",
    "# features = ['visibility', 'exposure']\n",
    "# features = ['objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy']\n",
    "X = data[features]\n",
    "y = data['mean_f2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "      <th>niqe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.083000e+03</td>\n",
       "      <td>6083.000000</td>\n",
       "      <td>6.083000e+03</td>\n",
       "      <td>6083.000000</td>\n",
       "      <td>6083.000000</td>\n",
       "      <td>6083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.777211e-01</td>\n",
       "      <td>0.231871</td>\n",
       "      <td>6.378834e-03</td>\n",
       "      <td>0.145793</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.134308e-01</td>\n",
       "      <td>0.269062</td>\n",
       "      <td>4.907808e-03</td>\n",
       "      <td>0.072428</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.024679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.222425e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.902979e-08</td>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.020507e-01</td>\n",
       "      <td>0.014917</td>\n",
       "      <td>2.376295e-03</td>\n",
       "      <td>0.096630</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.346791e-01</td>\n",
       "      <td>0.117202</td>\n",
       "      <td>5.489420e-03</td>\n",
       "      <td>0.105499</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.125931e-01</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>9.104459e-03</td>\n",
       "      <td>0.209953</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.695124e-01</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>2.989884e-02</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.056565</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         visibility     exposure  objectness_uncertainty  \\\n",
       "count  6.083000e+03  6083.000000            6.083000e+03   \n",
       "mean   2.777211e-01     0.231871            6.378834e-03   \n",
       "std    2.134308e-01     0.269062            4.907808e-03   \n",
       "min    3.222425e-07     0.000000            3.902979e-08   \n",
       "25%    1.020507e-01     0.014917            2.376295e-03   \n",
       "50%    2.346791e-01     0.117202            5.489420e-03   \n",
       "75%    4.125931e-01     0.386221            9.104459e-03   \n",
       "max    9.695124e-01     0.999207            2.989884e-02   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy         niqe  \n",
       "count            6083.000000       6083.000000  6083.000000  \n",
       "mean                0.145793          0.001423     0.001273  \n",
       "std                 0.072428          0.004295     0.024679  \n",
       "min                 0.050507          0.000000     0.000000  \n",
       "25%                 0.096630          0.000037     0.000033  \n",
       "50%                 0.105499          0.000158     0.000068  \n",
       "75%                 0.209953          0.000815     0.000137  \n",
       "max                 0.479202          0.056565     1.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.191435</td>\n",
       "      <td>0.100082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Regressor</th>\n",
       "      <td>0.277962</td>\n",
       "      <td>-0.306672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP Regressor</th>\n",
       "      <td>0.187570</td>\n",
       "      <td>0.118250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regressor</th>\n",
       "      <td>0.143621</td>\n",
       "      <td>0.324849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM Regressor</th>\n",
       "      <td>0.207569</td>\n",
       "      <td>0.024235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MSE        R2\n",
       "Linear Regression        0.191435  0.100082\n",
       "Decision Tree Regressor  0.277962 -0.306672\n",
       "MLP Regressor            0.187570  0.118250\n",
       "Random Forest Regressor  0.143621  0.324849\n",
       "SVM Regressor            0.207569  0.024235"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"MLP Regressor\": MLPRegressor(random_state=42, max_iter=1000), # Increased max_iter for convergence\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
    "    # SVM\n",
    "    \"SVM Regressor\": SVR(kernel='linear', C=1.0, epsilon=0.1),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\"MSE\": mse, \"R2\": r2}\n",
    "    \n",
    "    # if name == 'Decision Tree Regressor':\n",
    "    #     # save test resulst to csv\n",
    "    #     temp = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "    #     .to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/test_result.csv', index=False)\n",
    "\n",
    "results_df = pd.DataFrame(results).T  # Convert results to a DataFrame for better readability\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.13651,     0.14236,      0.1451,     0.20377,     0.26094,     0.11132])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
