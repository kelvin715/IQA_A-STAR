{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 crescent_gap, 1 welding_line, 1: 640x640 1 water_spot, 2: 640x640 1 oil_spot, 3: 640x640 1 water_spot, 4: 640x640 4 crescent_gaps, 1 welding_line, 5: 640x640 1 silk_spot, 6: 640x640 3 oil_spots, 3 punching_holes, 2 welding_lines, 7: 640x640 2 water_spots, 8: 640x640 1 waist_folding, 9: 640x640 1 waist_folding, 10: 640x640 1 crescent_gap, 1 welding_line, 11: 640x640 2 crescent_gaps, 12: 640x640 (no detections), 13: 640x640 (no detections), 14: 640x640 2 silk_spots, 15: 640x640 1 water_spot, 16: 640x640 2 oil_spots, 17: 640x640 1 silk_spot, 18: 640x640 2 oil_spots, 19: 640x640 1 crescent_gap, 2 welding_lines, 20: 640x640 1 crescent_gap, 1 welding_line, 21: 640x640 (no detections), 22: 640x640 1 punching_hole, 23: 640x640 1 punching_hole, 1 welding_line, 24: 640x640 1 punching_hole, 25: 640x640 1 silk_spot, 26: 640x640 1 punching_hole, 3 welding_lines, 27: 640x640 1 water_spot, 28: 640x640 1 water_spot, 29: 640x640 (no detections), 30: 640x640 1 silk_spot, 1 water_spot, 31: 640x640 1 water_spot, 32: 640x640 2 silk_spots, 33: 640x640 1 punching_hole, 34: 640x640 2 welding_lines, 35: 640x640 2 welding_lines, 36: 640x640 1 water_spot, 37: 640x640 1 punching_hole, 2 welding_lines, 38: 640x640 1 waist_folding, 39: 640x640 1 silk_spot, 40: 640x640 3 silk_spots, 41: 640x640 (no detections), 42: 640x640 1 punching_hole, 43: 640x640 (no detections), 44: 640x640 1 water_spot, 45: 640x640 1 crescent_gap, 3 oil_spots, 1 punching_hole, 46: 640x640 1 punching_hole, 1 welding_line, 47: 640x640 1 punching_hole, 2 welding_lines, 48: 640x640 1 water_spot, 49: 640x640 (no detections), 50: 640x640 4 oil_spots, 51: 640x640 (no detections), 52: 640x640 2 waist_foldings, 53: 640x640 5 oil_spots, 54: 640x640 1 crescent_gap, 1 welding_line, 55: 640x640 1 punching_hole, 1 welding_line, 56: 640x640 1 water_spot, 57: 640x640 1 waist_folding, 58: 640x640 (no detections), 59: 640x640 1 crescent_gap, 60: 640x640 2 silk_spots, 1 water_spot, 61: 640x640 1 water_spot, 62: 640x640 1 silk_spot, 63: 640x640 2 silk_spots, 64: 640x640 (no detections), 65: 640x640 2 water_spots, 66: 640x640 (no detections), 67: 640x640 1 oil_spot, 1 water_spot, 68: 640x640 2 oil_spots, 69: 640x640 (no detections), 70: 640x640 (no detections), 71: 640x640 (no detections), 72: 640x640 (no detections), 73: 640x640 1 crescent_gap, 1 welding_line, 74: 640x640 1 oil_spot, 75: 640x640 1 silk_spot, 76: 640x640 1 water_spot, 77: 640x640 1 silk_spot, 78: 640x640 1 silk_spot, 79: 640x640 (no detections), 80: 640x640 5 oil_spots, 81: 640x640 1 crescent_gap, 1 welding_line, 82: 640x640 2 silk_spots, 83: 640x640 2 crescent_gaps, 1 welding_line, 84: 640x640 1 water_spot, 85: 640x640 1 crescent_gap, 1 welding_line, 86: 640x640 1 silk_spot, 87: 640x640 (no detections), 88: 640x640 (no detections), 89: 640x640 1 crescent_gap, 1 welding_line, 90: 640x640 1 water_spot, 91: 640x640 (no detections), 92: 640x640 1 crescent_gap, 1 silk_spot, 1 welding_line, 93: 640x640 1 oil_spot, 1 rolled_pit, 94: 640x640 3 crescent_gaps, 95: 640x640 1 punching_hole, 96: 640x640 1 water_spot, 97: 640x640 (no detections), 98: 640x640 2 punching_holes, 1 silk_spot, 1 waist_folding, 1 welding_line, 99: 640x640 1 oil_spot, 100: 640x640 (no detections), 101: 640x640 (no detections), 102: 640x640 1 crescent_gap, 1 silk_spot, 103: 640x640 1 silk_spot, 104: 640x640 (no detections), 105: 640x640 2 silk_spots, 106: 640x640 1 silk_spot, 107: 640x640 1 welding_line, 108: 640x640 1 oil_spot, 109: 640x640 1 silk_spot, 110: 640x640 2 waist_foldings, 111: 640x640 1 welding_line, 112: 640x640 1 crescent_gap, 1 welding_line, 113: 640x640 1 waist_folding, 114: 640x640 1 silk_spot, 115: 640x640 1 silk_spot, 116: 640x640 2 water_spots, 117: 640x640 1 punching_hole, 2 welding_lines, 118: 640x640 5 silk_spots, 119: 640x640 1 crescent_gap, 2 welding_lines, 120: 640x640 4 silk_spots, 121: 640x640 (no detections), 122: 640x640 (no detections), 123: 640x640 1 silk_spot, 124: 640x640 1 silk_spot, 125: 640x640 (no detections), 126: 640x640 1 water_spot, 127: 640x640 (no detections), 128: 640x640 1 silk_spot, 129: 640x640 1 silk_spot, 130: 640x640 1 punching_hole, 131: 640x640 1 crescent_gap, 132: 640x640 2 oil_spots, 133: 640x640 2 oil_spots, 134: 640x640 2 silk_spots, 135: 640x640 1 waist_folding, 136: 640x640 2 crescent_gaps, 137: 640x640 1 punching_hole, 1 waist_folding, 1 welding_line, 138: 640x640 1 water_spot, 139: 640x640 (no detections), 140: 640x640 1 silk_spot, 1 water_spot, 141: 640x640 1 crescent_gap, 1 welding_line, 142: 640x640 2 silk_spots, 143: 640x640 1 silk_spot, 144: 640x640 2 silk_spots, 145: 640x640 2 silk_spots, 146: 640x640 (no detections), 147: 640x640 1 waist_folding, 148: 640x640 (no detections), 149: 640x640 (no detections), 150: 640x640 (no detections), 151: 640x640 (no detections), 152: 640x640 1 silk_spot, 153: 640x640 2 silk_spots, 154: 640x640 1 silk_spot, 155: 640x640 1 silk_spot, 156: 640x640 2 silk_spots, 157: 640x640 (no detections), 158: 640x640 1 punching_hole, 1 welding_line, 159: 640x640 1 crescent_gap, 160: 640x640 1 crescent_gap, 1 punching_hole, 1 welding_line, 161: 640x640 1 waist_folding, 162: 640x640 1 silk_spot, 163: 640x640 1 welding_line, 164: 640x640 1 silk_spot, 1 welding_line, 165: 640x640 2 crescent_gaps, 1 welding_line, 166: 640x640 1 punching_hole, 2 welding_lines, 167: 640x640 2 oil_spots, 168: 640x640 1 silk_spot, 169: 640x640 (no detections), 170: 640x640 1 silk_spot, 171: 640x640 1 silk_spot, 172: 640x640 1 punching_hole, 1 welding_line, 173: 640x640 1 silk_spot, 174: 640x640 1 silk_spot, 175: 640x640 (no detections), 176: 640x640 2 silk_spots, 177: 640x640 1 punching_hole, 1 welding_line, 178: 640x640 2 water_spots, 179: 640x640 1 welding_line, 180: 640x640 3 inclusions, 181: 640x640 1 welding_line, 182: 640x640 (no detections), 183: 640x640 1 welding_line, 184: 640x640 1 crescent_gap, 1 welding_line, 185: 640x640 1 oil_spot, 186: 640x640 1 crescent_gap, 1 welding_line, 187: 640x640 1 oil_spot, 188: 640x640 2 water_spots, 189: 640x640 1 crescent_gap, 1 oil_spot, 1 silk_spot, 1 welding_line, 190: 640x640 1 waist_folding, 191: 640x640 1 punching_hole, 192: 640x640 1 silk_spot, 193: 640x640 1 silk_spot, 194: 640x640 (no detections), 195: 640x640 (no detections), 196: 640x640 1 crescent_gap, 197: 640x640 1 welding_line, 198: 640x640 1 water_spot, 199: 640x640 1 crescent_gap, 2 welding_lines, 200: 640x640 1 punching_hole, 201: 640x640 1 crescent_gap, 2 welding_lines, 202: 640x640 (no detections), 203: 640x640 1 waist_folding, 204: 640x640 2 crescent_gaps, 205: 640x640 1 oil_spot, 206: 640x640 1 punching_hole, 1 welding_line, 207: 640x640 (no detections), 208: 640x640 1 welding_line, 209: 640x640 1 punching_hole, 210: 640x640 1 welding_line, 211: 640x640 1 water_spot, 212: 640x640 1 crescent_gap, 1 welding_line, 213: 640x640 1 crescent_gap, 2 welding_lines, 214: 640x640 2 water_spots, 215: 640x640 (no detections), 216: 640x640 2 water_spots, 217: 640x640 (no detections), 218: 640x640 1 water_spot, 219: 640x640 1 oil_spot, 1 water_spot, 220: 640x640 1 punching_hole, 1 welding_line, 221: 640x640 1 silk_spot, 222: 640x640 2 silk_spots, 223: 640x640 3 silk_spots, 224: 640x640 1 silk_spot, 225: 640x640 3 silk_spots, 226: 640x640 1 water_spot, 227: 640x640 1 inclusion, 1 water_spot, 228: 640x640 (no detections), 229: 640x640 1 waist_folding, 655.1ms\n",
      "Speed: 12.4ms preprocess, 2.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "#set visible cuda\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_brightness_0 detect by yolov8n with dropout(p=0.1)/weights/best.pt')\n",
    "\n",
    "# Define path to directory containing images and videos for inference\n",
    "source = '/Data4/student_zhihan_data/data/GC10-DET/test/images'\n",
    "\n",
    "\n",
    "# Run inference on the source\n",
    "results = model([os.path.join(source, i) for i in os.listdir(source)])# generator of Results objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import compute_ap\n",
    "from ultralytics.engine.validator import BaseValidator\n",
    "from ultralytics.utils.metrics import box_iou, Metric, DetMetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def smooth(y, f=0.05):\n",
    "    \"\"\"Box filter of fraction f.\"\"\"\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = np.ones(nf // 2)  # ones padding\n",
    "    yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return np.convolve(yp, np.ones(nf) / nf, mode=\"valid\")  # y-smoothed\n",
    "\n",
    "def ap_per_class(\n",
    "    tp, conf, pred_cls, target_cls, plot=False, on_plot=None, save_dir=Path(), names=(), eps=1e-16, prefix=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the average precision per class for object detection evaluation.\n",
    "\n",
    "    Args:\n",
    "        tp (np.ndarray): Binary array indicating whether the detection is correct (True) or not (False).\n",
    "        conf (np.ndarray): Array of confidence scores of the detections.\n",
    "        pred_cls (np.ndarray): Array of predicted classes of the detections.\n",
    "        target_cls (np.ndarray): Array of true classes of the detections.\n",
    "        plot (bool, optional): Whether to plot PR curves or not. Defaults to False.\n",
    "        on_plot (func, optional): A callback to pass plots path and data when they are rendered. Defaults to None.\n",
    "        save_dir (Path, optional): Directory to save the PR curves. Defaults to an empty path.\n",
    "        names (tuple, optional): Tuple of class names to plot PR curves. Defaults to an empty tuple.\n",
    "        eps (float, optional): A small value to avoid division by zero. Defaults to 1e-16.\n",
    "        prefix (str, optional): A prefix string for saving the plot files. Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): A tuple of six arrays and one array of unique classes, where:\n",
    "            tp (np.ndarray): True positive counts at threshold given by max F1 metric for each class.Shape: (nc,).\n",
    "            fp (np.ndarray): False positive counts at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            p (np.ndarray): Precision values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            r (np.ndarray): Recall values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            f1 (np.ndarray): F1-score values at threshold given by max F1 metric for each class. Shape: (nc,).\n",
    "            ap (np.ndarray): Average precision for each class at different IoU thresholds. Shape: (nc, 10).\n",
    "            unique_classes (np.ndarray): An array of unique classes that have data. Shape: (nc,).\n",
    "            p_curve (np.ndarray): Precision curves for each class. Shape: (nc, 1000).\n",
    "            r_curve (np.ndarray): Recall curves for each class. Shape: (nc, 1000).\n",
    "            f1_curve (np.ndarray): F1-score curves for each class. Shape: (nc, 1000).\n",
    "            x (np.ndarray): X-axis values for the curves. Shape: (1000,).\n",
    "            prec_values: Precision values at mAP@0.5 for each class. Shape: (nc, 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = np.unique(target_cls, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    x, prec_values = np.linspace(0, 1, 1000), []\n",
    "\n",
    "    # Average precision, precision and recall curves\n",
    "    ap, p_curve, r_curve = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = pred_cls == c\n",
    "        n_l = nt[ci]  # number of labels\n",
    "        n_p = i.sum()  # number of predictions\n",
    "        if n_p == 0 or n_l == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (n_l + eps)  # recall curve\n",
    "        r_curve[ci] = np.interp(-x, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p_curve[ci] = np.interp(-x, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
    "            if plot and j == 0:\n",
    "                prec_values.append(np.interp(x, mrec, mpre))  # precision at mAP@0.5\n",
    "\n",
    "    prec_values = np.array(prec_values)  # (nc, 1000)\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1_curve = 2 * p_curve * r_curve / (p_curve + r_curve + eps)\n",
    "    f2_curve = (5 * p_curve * r_curve) / (4 * p_curve + r_curve + eps)\n",
    "    # names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n",
    "    # names = dict(enumerate(names))  # to dict\n",
    "    # if plot:\n",
    "    #     plot_pr_curve(x, prec_values, ap, save_dir / f\"{prefix}PR_curve.png\", names, on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, f1_curve, save_dir / f\"{prefix}F1_curve.png\", names, ylabel=\"F1\", on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, p_curve, save_dir / f\"{prefix}P_curve.png\", names, ylabel=\"Precision\", on_plot=on_plot)\n",
    "    #     plot_mc_curve(x, r_curve, save_dir / f\"{prefix}R_curve.png\", names, ylabel=\"Recall\", on_plot=on_plot)\n",
    "\n",
    "    i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1, f2 = p_curve[:, i], r_curve[:, i], f1_curve[:, i], f2_curve[:, i]  # max-F1 precision, recall, F1 values\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    \n",
    "    ap_50 = ap[:, 0].mean()\n",
    "    ap_50_95 = ap.mean()\n",
    "    \n",
    "    return tp, fp, p, r, f1, f2, ap_50, ap_50_95, unique_classes.astype(int), p_curve, r_curve, f1_curve, f2_curve, x, prec_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f2(source, model):\n",
    "    results = model([os.path.join(source, i) for i in os.listdir(source)])\n",
    "    validator = BaseValidator()\n",
    "    validator.iouv = torch.arange(0.5, 1, 0.05)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['img_name', 'p', 'r', 'f1', 'f2', 'ap_50', 'ap_50_95', 'conf', 'pred_cls', 'target_cls'])\n",
    "\n",
    "    for result in results:\n",
    "        metric = DetMetrics()\n",
    "        label_path = result.path.replace('images', 'labels')[:-4] + '.txt'\n",
    "        label = torch.from_numpy(np.loadtxt(label_path))\n",
    "        # turn label into (x1, y1, x2, y2) format\n",
    "        \n",
    "        if len(label) == 0:\n",
    "            continue\n",
    "        \n",
    "        if label.dim() > 1:\n",
    "            x1 = label[:, 1] - label[:, 3] / 2\n",
    "            y1 = label[:, 2] - label[:, 4] / 2\n",
    "            x2 = label[:, 1] + label[:, 3] / 2\n",
    "            y2 = label[:, 2] + label[:, 4] / 2\n",
    "            label = torch.stack((label[:, 0], x1, y1, x2, y2), 1)\n",
    "            iou = box_iou(label[:, 1:].to(\"cuda:0\"), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "            tp = validator.match_predictions(result.boxes.data[:,-1], label[:, 0].to(\"cuda:0\"), iou)\n",
    "            \n",
    "            # update metric\n",
    "            tp = tp.detach().cpu().numpy()\n",
    "            conf = result.boxes.conf.detach().cpu().numpy()\n",
    "            pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "            target_cls = label[:, 0].detach().cpu().numpy()\n",
    "        \n",
    "        elif label.dim() == 1:\n",
    "            x1 = label[1] - label[3] / 2\n",
    "            y1 = label[2] - label[4] / 2\n",
    "            x2 = label[1] + label[3] / 2\n",
    "            y2 = label[2] + label[4] / 2\n",
    "            label = torch.tensor([label[0], x1, y1, x2, y2])\n",
    "            iou = box_iou(label[1:].to(\"cuda:0\").reshape(1, -1), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "            tp = validator.match_predictions(result.boxes.data[:,-1], label[0].to(\"cuda:0\").unsqueeze(0), iou)\n",
    "        \n",
    "            # update metric\n",
    "            tp = tp.detach().cpu().numpy()\n",
    "            conf = result.boxes.conf.detach().cpu().numpy()\n",
    "            pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "            target_cls = label[0].unsqueeze(0).detach().cpu().numpy()\n",
    "            \n",
    "        _, _, p, r, f1, f2, ap_50, ap_50_95, unique_classes, p_curve, r_curve, f1_curve, f2_curve, x, prec_values = ap_per_class(tp, conf, pre_cls, target_cls)\n",
    "        print(f2.mean(), unique_classes)\n",
    "        \n",
    "        # update df\n",
    "        df.loc[len(df)] = [result.path, p, r, f1, f2, ap_50, ap_50_95, conf, pre_cls, target_cls]\n",
    "\n",
    "    df.to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record/{source.split(\"/\")[-3]}_new.csv', index=False, header=True)\n",
    "    return df\n",
    "  \n",
    "\n",
    "for i in os.listdir('/Data4/student_zhihan_data/data'):\n",
    "    if i[-3:] != 'csv' and i != 'NEU-DET' and i != 'data.zip':\n",
    "        source = os.path.join('/Data4/student_zhihan_data/data', i, 'test/images')\n",
    "        df = compute_f2(source, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: uncertainty * 3, proposed * score, NIQE, BRISQUE\n",
    "# output: quality score \n",
    "\n",
    "F2_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record'\n",
    "Proposed_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Score_Record'\n",
    "Uncertainty_dir = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record'\n",
    "\n",
    "# concat all csv\n",
    "dfs = []\n",
    "for idx, dir in enumerate([F2_dir, Proposed_dir, Uncertainty_dir]):\n",
    "    files = os.listdir(dir)\n",
    "    \n",
    "    if idx == 0:\n",
    "        continue     \n",
    "        # df.to_csv(f'{dir}_new.csv', index=False, header=True)\n",
    "        for file in files:\n",
    "            if file[-7:] == 'new.csv':\n",
    "                df = pd.concat([df, pd.read_csv(os.path.join(dir, file))], ignore_index=True)\n",
    "            \n",
    "    elif idx == 1:\n",
    "        df = pd.DataFrame(columns=['dataset','img_name','p', 'r', 'f1', 'f2', 'ap_50', 'ap_50_95', 'conf', 'pred_cls', 'target_cls'])\n",
    "        for file in files:\n",
    "            if file[-3:] == 'csv':\n",
    "                df = pd.concat([df, pd.read_csv(os.path.join(dir, file))], ignore_index=True)\n",
    "        df['img_name'] = df['dataset'] + '/images/' + df['img_name']\n",
    "        df = df.drop(columns=['dataset'])\n",
    "        # df.to_csv(f'{dir}.csv', index=False, header=True)\n",
    "    else:\n",
    "        for file in files:\n",
    "            if file[-3:] == 'csv':\n",
    "                df = pd.concat([df, pd.read_csv(os.path.join(dir, file))], ignore_index=True)\n",
    "        df['img_name'] = df['dataset'] + '/test/images/' + df['img_name']\n",
    "        df = df.drop(columns=['dataset'])\n",
    "        # df.to_csv(f'{dir}.csv', index=False, header=True)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine\n",
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv')\n",
    "df['img_name'] = df['dataset'] + '/test/images/' + df['img_name']\n",
    "df = df.drop(columns=['dataset'])\n",
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv', index=False, header=True)\n",
    "\n",
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv')\n",
    "df['img_name'] = df['dataset'] + '/images/' + df['img_name']\n",
    "df = df.drop(columns=['dataset'])\n",
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2732998/2401473654.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, tmp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(Uncertainty_dir)\n",
    "df = pd.DataFrame(columns=['dataset','img_name','objectness_uncertainty','objectness_entropy','weighted_variance_sum','weighted_entropy'])\n",
    "for file in files:\n",
    "    if file[-3:] == 'csv':\n",
    "        tmp = pd.read_csv(os.path.join(Uncertainty_dir, file))\n",
    "        #add header to tmp\n",
    "        tmp.columns = ['dataset','img_name','objectness_uncertainty','objectness_entropy','weighted_variance_sum','weighted_entropy']\n",
    "        df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "df.to_csv(f'Uncertainty_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2732998/2492081121.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, tmp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(Proposed_dir)\n",
    "df = pd.DataFrame(columns=['dataset', 'img_name', 'visibility', 'exposure'])\n",
    "for file in files:\n",
    "    if file[-3:] == 'csv':\n",
    "        tmp = pd.read_csv(os.path.join(Proposed_dir, file))\n",
    "        #add header to tmp\n",
    "        tmp.columns = ['dataset', 'img_name', 'visibility', 'exposure']\n",
    "        df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "df.to_csv(f'Proposed_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for idx, dir in enumerate(['/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/F2_Record_new.csv',\n",
    "                          '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv',\n",
    "                           '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv']):\n",
    "    df = pd.read_csv(dir)\n",
    "    dfs.append(df)\n",
    " \n",
    "merged_df = pd.merge(dfs[0], dfs[1], on=['img_name'])\n",
    "merged_df = pd.merge(merged_df, dfs[2], on=['img_name'])\n",
    "    \n",
    "    \n",
    "# columns = set()\n",
    "# for df in dfs:\n",
    "#     columns = columns.union(set(df.columns))\n",
    "\n",
    "# df = pd.DataFrame(columns=list(columns))\n",
    "# img_name_union = set(dfs[0]['img_name'])\n",
    "\n",
    "# for tmp in dfs:\n",
    "#     img_name_union = img_name_union.intersection(set(tmp['img_name']))\n",
    "\n",
    "# for tmp in dfs:\n",
    "#     merged_df = pd.merge(df, tmp, on=['img_name'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ap_50</th>\n",
       "      <th>ap_50_95</th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7064.000000</td>\n",
       "      <td>7064.000000</td>\n",
       "      <td>7.064000e+03</td>\n",
       "      <td>7064.000000</td>\n",
       "      <td>7.064000e+03</td>\n",
       "      <td>7064.000000</td>\n",
       "      <td>7064.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.475281</td>\n",
       "      <td>0.239499</td>\n",
       "      <td>2.669650e-01</td>\n",
       "      <td>0.202524</td>\n",
       "      <td>6.347572e-03</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.144991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466809</td>\n",
       "      <td>0.275075</td>\n",
       "      <td>2.080242e-01</td>\n",
       "      <td>0.260773</td>\n",
       "      <td>4.891388e-03</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.071955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.222425e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.902979e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.706808e-02</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>2.335405e-03</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.096591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>2.238936e-01</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>5.465690e-03</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.105462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.460437</td>\n",
       "      <td>3.947939e-01</td>\n",
       "      <td>0.326185</td>\n",
       "      <td>9.125920e-03</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.205264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>9.695124e-01</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>2.989884e-02</td>\n",
       "      <td>0.056565</td>\n",
       "      <td>0.479202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ap_50     ap_50_95    visibility     exposure  \\\n",
       "count  7064.000000  7064.000000  7.064000e+03  7064.000000   \n",
       "mean      0.475281     0.239499  2.669650e-01     0.202524   \n",
       "std       0.466809     0.275075  2.080242e-01     0.260773   \n",
       "min       0.000000     0.000000  3.222425e-07     0.000000   \n",
       "25%       0.000000     0.000000  9.706808e-02     0.006841   \n",
       "50%       0.497500     0.099500  2.238936e-01     0.075973   \n",
       "75%       0.995000     0.460437  3.947939e-01     0.326185   \n",
       "max       0.995000     0.995000  9.695124e-01     0.999207   \n",
       "\n",
       "       objectness_uncertainty  weighted_variance_sum  weighted_entropy  \n",
       "count            7.064000e+03            7064.000000       7064.000000  \n",
       "mean             6.347572e-03               0.001332          0.144991  \n",
       "std              4.891388e-03               0.004060          0.071955  \n",
       "min              3.902979e-08               0.000000          0.050507  \n",
       "25%              2.335405e-03               0.000038          0.096591  \n",
       "50%              5.465690e-03               0.000169          0.105462  \n",
       "75%              9.125920e-03               0.000804          0.205264  \n",
       "max              2.989884e-02               0.056565          0.479202  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['weighted_entropy'] = merged_df['weighted_variance_sum']\n",
    "merged_df['weighted_variance_sum'] = merged_df['objectness_entropy']\n",
    "merged_df.drop(columns=['objectness_entropy'], inplace=True)\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged_new.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv')\n",
    "a =  df['img_name'].apply(lambda x: '/'.join(x.split('/')[:-2]))\n",
    "b = df['img_name'].apply(lambda x: '/'.join(x.split('/')[-2:]))\n",
    "\n",
    "#concat a, '/images/, b to a new column\n",
    "df['img_name'] = a + '/test/' + b\n",
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Uncertainty_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv')\n",
    "df['img_name'] = df['dataset'] + '/images/' + df['img_name']\n",
    "df = df.drop(columns=['dataset'])\n",
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/Proposed_Record.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 [1 9]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "1.0 [8]\n",
      "0.8125 [1 9]\n",
      "1.0 [6]\n",
      "0.0 [0]\n",
      "0.0 [8]\n",
      "1.0 [7]\n",
      "0.0 [7]\n",
      "1.0 [1 9]\n",
      "0.8333333333333334 [1]\n",
      "0.0 [2]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [5]\n",
      "0.8861831775579442 [3]\n",
      "0.0 [8]\n",
      "0.5 [3]\n",
      "0.9894106530883233 [1 9]\n",
      "1.0 [1 9]\n",
      "0.0 [3]\n",
      "1.0 [4]\n",
      "0.0 [9]\n",
      "0.25 [1 4 8 9]\n",
      "1.0 [6]\n",
      "0.955061678002223 [4 9]\n",
      "1.0 [8]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "1.0 [6 8]\n",
      "1.0 [8]\n",
      "0.0 [6]\n",
      "1.0 [4]\n",
      "0.9698366958523534 [9]\n",
      "0.8333333333333334 [9]\n",
      "1.0 [8]\n",
      "0.9166666666666667 [4 9]\n",
      "1.0 [7]\n",
      "1.0 [6]\n",
      "0.9784231710625808 [6]\n",
      "0.0 [6]\n",
      "1.0 [4]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [2]\n",
      "1.0 [4 9]\n",
      "0.969202609172539 [4 9]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "0.6682954202760385 [3]\n",
      "0.0 [6]\n",
      "0.9784407998647183 [7]\n",
      "0.8620689655172415 [3]\n",
      "1.0 [1 9]\n",
      "1.0 [4 9]\n",
      "0.0 [2]\n",
      "1.0 [7]\n",
      "0.0 [0]\n",
      "0.5 [1 9]\n",
      "0.9166666666666667 [6 8]\n",
      "1.0 [8]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [7]\n",
      "0.9704364934628894 [8]\n",
      "0.0 [2]\n",
      "0.0 [5]\n",
      "0.5 [3]\n",
      "0.0 [6]\n",
      "0.0 [2 6]\n",
      "0.0 [2]\n",
      "0.0 [6]\n",
      "1.0 [1]\n",
      "1.0 [3]\n",
      "1.0 [6]\n",
      "0.0 [6 8]\n",
      "0.0 [6]\n",
      "0.0 [5]\n",
      "0.0 [7]\n",
      "0.8282790301542053 [3]\n",
      "1.0 [9]\n",
      "0.0 [2]\n",
      "0.9166666666666667 [1 9]\n",
      "1.0 [8]\n",
      "1.0 [1 9]\n",
      "1.0 [6]\n",
      "0.0 [3]\n",
      "0.0 [2 8]\n",
      "1.0 [9]\n",
      "1.0 [8]\n",
      "0.0 [9]\n",
      "1.0 [9]\n",
      "0.0 [5]\n",
      "0.969870198914854 [1]\n",
      "1.0 [4]\n",
      "1.0 [8]\n",
      "0.0 [6]\n",
      "0.9166666666666667 [4 9]\n",
      "0.0 [8]\n",
      "0.0 [2 6]\n",
      "0.0 [3]\n",
      "1.0 [1]\n",
      "0.5 [6 9]\n",
      "0.0 [6]\n",
      "0.0 [8]\n",
      "1.0 [6]\n",
      "0.3333333333333333 [4 5 9]\n",
      "1.0 [3]\n",
      "1.0 [6]\n",
      "0.8333333333333334 [7]\n",
      "1.0 [9]\n",
      "1.0 [1 9]\n",
      "0.0 [7]\n",
      "1.0 [6]\n",
      "0.0 [2]\n",
      "0.8333333333333334 [8]\n",
      "0.48425917805839425 [0 3 4 9]\n",
      "0.0 [6]\n",
      "0.9664022636197249 [1 9]\n",
      "0.625 [6]\n",
      "0.0 [2 8]\n",
      "0.0 [8]\n",
      "1.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6 8]\n",
      "1.0 [8]\n",
      "0.0 [6]\n",
      "1.0 [6]\n",
      "0.5555555555555556 [6]\n",
      "0.5 [4 6]\n",
      "1.0 [1]\n",
      "0.9383478324058097 [3]\n",
      "0.2777777777777778 [3]\n",
      "0.915059415479969 [6]\n",
      "1.0 [7]\n",
      "0.966537216274089 [1]\n",
      "1.0 [4 9]\n",
      "1.0 [8]\n",
      "0.0 [2]\n",
      "1.0 [8]\n",
      "1.0 [9]\n",
      "0.8333333333333334 [6]\n",
      "1.0 [6]\n",
      "0.8333333333333334 [6]\n",
      "0.9538502012242512 [6]\n",
      "0.0 [6]\n",
      "1.0 [7]\n",
      "0.0 [6]\n",
      "0.0 [2]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.5555555555555556 [6]\n",
      "0.9761161289246528 [6]\n",
      "0.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [8]\n",
      "1.0 [4 9]\n",
      "1.0 [1]\n",
      "1.0 [1 9]\n",
      "1.0 [7]\n",
      "1.0 [6]\n",
      "1.0 [9]\n",
      "0.6666666666666666 [4 6 9]\n",
      "1.0 [9]\n",
      "1.0 [4]\n",
      "0.9817342520366794 [3]\n",
      "1.0 [6]\n",
      "0.0 [3 6]\n",
      "1.0 [6]\n",
      "0.0 [8]\n",
      "1.0 [4 9]\n",
      "1.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.8333333333333334 [6]\n",
      "1.0 [4 9]\n",
      "0.0 [3]\n",
      "0.5 [4 9]\n",
      "0.3333333333333333 [2]\n",
      "0.3333333333333333 [1 3 9]\n",
      "0.0 [2]\n",
      "1.0 [9]\n",
      "1.0 [1]\n",
      "0.5555555555555556 [3]\n",
      "1.0 [1]\n",
      "0.0 [8]\n",
      "0.4166666666666667 [6 8]\n",
      "1.0 [1]\n",
      "0.0 [7]\n",
      "0.0 [4]\n",
      "1.0 [6]\n",
      "1.0 [6]\n",
      "0.0 [6]\n",
      "0.0 [2]\n",
      "1.0 [1]\n",
      "1.0 [9]\n",
      "1.0 [8]\n",
      "0.9738349911183414 [1 9]\n",
      "0.5 [4 8]\n",
      "0.9674976704199053 [9]\n",
      "0.0 [5]\n",
      "0.0 [7]\n",
      "0.9583904309214064 [1]\n",
      "0.3846153846153846 [3]\n",
      "1.0 [4 9]\n",
      "0.0 [2]\n",
      "0.0 [0]\n",
      "1.0 [4]\n",
      "0.0 [9]\n",
      "0.5 [6 8]\n",
      "1.0 [1 9]\n",
      "0.9606150748563909 [1 9]\n",
      "0.9107177457645993 [8]\n",
      "0.0 [3]\n",
      "0.0 [6]\n",
      "0.0 [6]\n",
      "0.5555555555555556 [8]\n",
      "1.0 [8]\n",
      "1.0 [4 9]\n",
      "0.5555555555555556 [6]\n",
      "0.8333333333333334 [6]\n",
      "0.9796323665356054 [6]\n",
      "0.0 [6]\n",
      "0.7952832871373511 [6]\n",
      "1.0 [8]\n",
      "1.0 [2]\n",
      "0.0 [9]\n",
      "1.0 [7]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.engine.validator import BaseValidator\n",
    "from ultralytics.utils.metrics import box_iou, Metric, DetMetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# load txt label\n",
    "\n",
    "\n",
    "validator = BaseValidator()\n",
    "validator.iouv = torch.arange(0.5, 1, 0.05)\n",
    "\n",
    "for result in results:\n",
    "    metric = DetMetrics()\n",
    "    label_path = result.path.replace('images', 'labels')[:-4] + '.txt'\n",
    "    label = torch.from_numpy(np.loadtxt(label_path))\n",
    "    # turn label into (x1, y1, x2, y2) format\n",
    "    \n",
    "    if label.dim() > 1:\n",
    "        x1 = label[:, 1] - label[:, 3] / 2\n",
    "        y1 = label[:, 2] - label[:, 4] / 2\n",
    "        x2 = label[:, 1] + label[:, 3] / 2\n",
    "        y2 = label[:, 2] + label[:, 4] / 2\n",
    "        label = torch.stack((label[:, 0], x1, y1, x2, y2), 1)\n",
    "        iou = box_iou(label[:, 1:].to(\"cuda:0\"), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "        tp = validator.match_predictions(result.boxes.data[:,-1], label[:, 0].to(\"cuda:0\"), iou)\n",
    "        \n",
    "        # update metric\n",
    "        tp = tp.detach().cpu().numpy()\n",
    "        conf = result.boxes.conf.detach().cpu().numpy()\n",
    "        pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "        target_cls = label[:, 0].detach().cpu().numpy()\n",
    "    \n",
    "    else:\n",
    "        x1 = label[1] - label[3] / 2\n",
    "        y1 = label[2] - label[4] / 2\n",
    "        x2 = label[1] + label[3] / 2\n",
    "        y2 = label[2] + label[4] / 2\n",
    "        label = torch.tensor([label[0], x1, y1, x2, y2])\n",
    "        iou = box_iou(label[1:].to(\"cuda:0\").reshape(1, -1), result.boxes.xyxyn.to(\"cuda:0\"))\n",
    "        tp = validator.match_predictions(result.boxes.data[:,-1], label[0].to(\"cuda:0\").unsqueeze(0), iou)\n",
    "    \n",
    "        # update metric\n",
    "        tp = tp.detach().cpu().numpy()\n",
    "        conf = result.boxes.conf.detach().cpu().numpy()\n",
    "        pre_cls = result.boxes.cls.detach().cpu().numpy()\n",
    "        target_cls = label[0].unsqueeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    _, _, p, r, f1, f2, ap50, ap_50_95, unique_classes, p_curve, r_curve, f1_curve, f2_curve, x, prec_values = ap_per_class(tp, conf, pre_cls, target_cls)\n",
    "    print(f2.mean(), unique_classes)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results generator\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bbox outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def safe_convert_to_list(s):\n",
    "    try:\n",
    "        # Attempt to directly evaluate the string\n",
    "        return ast.literal_eval(s)\n",
    "    except SyntaxError:\n",
    "        # If direct evaluation fails, attempt to manually parse the string\n",
    "        cleaned_str = s.strip('[]')\n",
    "        if cleaned_str:  # Check if the string is not empty\n",
    "            numbers = [float(num) for num in cleaned_str.split() if num not in ['[', ']']]\n",
    "            return numbers\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged_new.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "data.head()\n",
    "\n",
    "# Apply the conversion function to the 'f2' column and then compute the mean F2 score\n",
    "data['f2'] = data['f2'].apply(safe_convert_to_list)\n",
    "data['mean_f2'] = data['f2'].apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "\n",
    "# Prepare the dataset for modeling\n",
    "features = ['visibility', 'exposure', 'objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy']\n",
    "# features = ['visibility', 'exposure']\n",
    "# features = ['objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy']\n",
    "X = data[features]\n",
    "# y = data['mean_f2']\n",
    "y = data['ap_50_95']\n",
    "\n",
    "# Display the first few rows of features and target to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.065377</td>\n",
       "      <td>0.131179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Regressor</th>\n",
       "      <td>0.096486</td>\n",
       "      <td>-0.282236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP Regressor</th>\n",
       "      <td>0.068393</td>\n",
       "      <td>0.091110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regressor</th>\n",
       "      <td>0.047503</td>\n",
       "      <td>0.368724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MSE        R2\n",
       "Linear Regression        0.065377  0.131179\n",
       "Decision Tree Regressor  0.096486 -0.282236\n",
       "MLP Regressor            0.068393  0.091110\n",
       "Random Forest Regressor  0.047503  0.368724"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"MLP Regressor\": MLPRegressor(random_state=42, max_iter=1000), # Increased max_iter for convergence\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\"MSE\": mse, \"R2\": r2}\n",
    "    \n",
    "    if name == 'Decision Tree Regressor':\n",
    "        # save test resulst to csv\n",
    "        df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "        df.to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/test_result.csv', index=False)\n",
    "\n",
    "results_df = pd.DataFrame(results).T  # Convert results to a DataFrame for better readability\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.197254</td>\n",
    "      <td>0.064327</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.380059</td>\n",
    "      <td>-0.802814</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.193648</td>\n",
    "      <td>0.081430</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.213226</td>\n",
    "      <td>-0.011437</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.199501</td>\n",
    "      <td>0.053668</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.283149</td>\n",
    "      <td>-0.343119</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.200023</td>\n",
    "      <td>0.051190</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.161334</td>\n",
    "      <td>0.234711</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>MSE</th>\n",
    "      <th>R2</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Linear Regression</th>\n",
    "      <td>0.188070</td>\n",
    "      <td>0.107889</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Decision Tree Regressor</th>\n",
    "      <td>0.275943</td>\n",
    "      <td>-0.308935</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>MLP Regressor</th>\n",
    "      <td>0.185389</td>\n",
    "      <td>0.120607</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Random Forest Regressor</th>\n",
    "      <td>0.140655</td>\n",
    "      <td>0.332804</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.16596,     0.16575,     0.17716,     0.26334,     0.22779])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.867000e+03</td>\n",
       "      <td>4867.000000</td>\n",
       "      <td>4.867000e+03</td>\n",
       "      <td>4867.000000</td>\n",
       "      <td>4867.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.796616e-01</td>\n",
       "      <td>0.232434</td>\n",
       "      <td>6.412672e-03</td>\n",
       "      <td>0.144884</td>\n",
       "      <td>0.001363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.144880e-01</td>\n",
       "      <td>0.268423</td>\n",
       "      <td>4.923872e-03</td>\n",
       "      <td>0.072237</td>\n",
       "      <td>0.004153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.222425e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.274079e-07</td>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.034640e-01</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>2.396813e-03</td>\n",
       "      <td>0.096459</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.356180e-01</td>\n",
       "      <td>0.119043</td>\n",
       "      <td>5.491689e-03</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>0.000154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.143271e-01</td>\n",
       "      <td>0.384620</td>\n",
       "      <td>9.171929e-03</td>\n",
       "      <td>0.203721</td>\n",
       "      <td>0.000790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.695124e-01</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>2.989884e-02</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.056565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         visibility     exposure  objectness_uncertainty  \\\n",
       "count  4.867000e+03  4867.000000            4.867000e+03   \n",
       "mean   2.796616e-01     0.232434            6.412672e-03   \n",
       "std    2.144880e-01     0.268423            4.923872e-03   \n",
       "min    3.222425e-07     0.000000            1.274079e-07   \n",
       "25%    1.034640e-01     0.016136            2.396813e-03   \n",
       "50%    2.356180e-01     0.119043            5.491689e-03   \n",
       "75%    4.143271e-01     0.384620            9.171929e-03   \n",
       "max    9.695124e-01     0.999207            2.989884e-02   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy  \n",
       "count            4867.000000       4867.000000  \n",
       "mean                0.144884          0.001363  \n",
       "std                 0.072237          0.004153  \n",
       "min                 0.050507          0.000000  \n",
       "25%                 0.096459          0.000037  \n",
       "50%                 0.105285          0.000154  \n",
       "75%                 0.203721          0.000790  \n",
       "max                 0.479202          0.056565  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1.217000e+03</td>\n",
       "      <td>1217.000000</td>\n",
       "      <td>1217.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.270493</td>\n",
       "      <td>0.230253</td>\n",
       "      <td>6.240577e-03</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.001661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.209810</td>\n",
       "      <td>0.272485</td>\n",
       "      <td>4.841622e-03</td>\n",
       "      <td>0.073080</td>\n",
       "      <td>0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.902979e-08</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.093177</td>\n",
       "      <td>0.010945</td>\n",
       "      <td>2.279311e-03</td>\n",
       "      <td>0.097146</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.230777</td>\n",
       "      <td>0.109753</td>\n",
       "      <td>5.438593e-03</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.402377</td>\n",
       "      <td>0.393562</td>\n",
       "      <td>8.849673e-03</td>\n",
       "      <td>0.226471</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.958951</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>2.272954e-02</td>\n",
       "      <td>0.413786</td>\n",
       "      <td>0.040970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        visibility     exposure  objectness_uncertainty  \\\n",
       "count  1217.000000  1217.000000            1.217000e+03   \n",
       "mean      0.270493     0.230253            6.240577e-03   \n",
       "std       0.209810     0.272485            4.841622e-03   \n",
       "min       0.000420     0.000000            3.902979e-08   \n",
       "25%       0.093177     0.010945            2.279311e-03   \n",
       "50%       0.230777     0.109753            5.438593e-03   \n",
       "75%       0.402377     0.393562            8.849673e-03   \n",
       "max       0.958951     0.999026            2.272954e-02   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy  \n",
       "count            1217.000000       1217.000000  \n",
       "mean                0.149462          0.001661  \n",
       "std                 0.073080          0.004813  \n",
       "min                 0.055160          0.000000  \n",
       "25%                 0.097146          0.000037  \n",
       "50%                 0.107024          0.000173  \n",
       "75%                 0.226471          0.000954  \n",
       "max                 0.413786          0.040970  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5651.000000\n",
       "mean        0.240280\n",
       "std         0.275259\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.099500\n",
       "75%         0.466655\n",
       "max         0.995000\n",
       "Name: ap_50_95, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1413.000000\n",
       "mean        0.236373\n",
       "std         0.274412\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.099500\n",
       "75%         0.447750\n",
       "max         0.995000\n",
       "Name: ap_50_95, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyiqa.archs.niqe_arch import *\n",
    "from pyiqa.utils import load_file_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model_urls = {\n",
    "    'url': 'https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/niqe_modelparameters.mat',\n",
    "    'niqe': 'https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/niqe_modelparameters.mat',\n",
    "    'ilniqe': 'https://github.com/chaofengc/IQA-PyTorch/releases/download/v0.1-weights/ILNIQE_templateModel.mat',\n",
    "    'pretrained': '/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1259.6615, dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "path = '/Data4/student_zhihan_data/data/GC10-DET_brightness_110/test/images/img_01_425000300_00630_jpg.rf.12001adc8b86faf88a47b6aa6f321b91.jpg'\n",
    "img = cv2.imread(path)\n",
    "img = torch.from_numpy(img)\n",
    "img = img.permute(2,0,1).unsqueeze(0)\n",
    "# calculate_niqe(img, color_space='gray', pretrained_model_path=load_file_from_url(default_model_urls['niqe']))\n",
    "# calculate_ilniqe(img, color_space='gray', pretrained_model_path=default_model_urls['pretrained'])\n",
    "calculate_niqe(img, color_space='gray', pretrained_model_path='model.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matlab model\n",
    "from scipy.io import loadmat\n",
    "covar = loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/covar.mat')\n",
    "mean = loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/mean.mat')\n",
    "model = loadmat(load_file_from_url(default_model_urls['niqe']))\n",
    "model['mu_prisparam'] = np.array(mean['mean'])\n",
    "model['cov_prisparam'] = np.array(covar['covariance'])\n",
    "# save mat model\n",
    "import scipy.io\n",
    "scipy.io.savemat('model.mat', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat')\n",
    "# change model key name 'None' to 'templateModel'\n",
    "model['templateModel'] = model.pop('None')\n",
    "scipy.io.savemat('model.mat', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Feb 14 16:14:01 2024',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'mu_prisparam': array([[     2.8918,      1.1237,     0.94071,  -0.0074117,     0.30461,     0.29457,     0.92068,     0.16218,     0.21715,     0.36395,     0.93394,   -0.093931,     0.33724,     0.25074,       0.934,   -0.094519,     0.33749,     0.25046,      2.7161,       1.036,     0.91893,    0.018017,     0.24276,\n",
       "             0.25803,     0.90293,     0.12705,     0.18806,     0.29925,     0.89492,   -0.053007,     0.26442,     0.21893,     0.89485,   -0.055262,     0.26535,     0.21794]]),\n",
       " 'cov_prisparam': array([[    0.11128,     0.04245,    0.031441, ...,   -0.003672,    0.022724,    0.018208],\n",
       "        [    0.04245,    0.022494,    0.013837, ...,  -0.0016413,    0.010588,   0.0086856],\n",
       "        [   0.031441,    0.013837,   0.0098078, ...,  -0.0011065,   0.0070665,   0.0057277],\n",
       "        ...,\n",
       "        [  -0.003672,  -0.0016413,  -0.0011065, ...,  0.00049509,  -0.0010687, -0.00059319],\n",
       "        [   0.022724,    0.010588,   0.0070665, ...,  -0.0010687,   0.0068859,   0.0055151],\n",
       "        [   0.018208,   0.0086856,   0.0057277, ..., -0.00059319,   0.0055151,   0.0046443]])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadmat('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri Aug 24 17:52:00 2012',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'mu_prisparam': array([[     2.6013,      0.9057,     0.81205,    0.090427,     0.13873,     0.20603,     0.81897,    0.062462,     0.15333,     0.19591,     0.82647,   -0.025526,     0.18857,     0.16578,     0.82429,   -0.025361,     0.18724,     0.16505,      2.9695,     0.96123,     0.84935,    0.082383,     0.16132,\n",
       "             0.22492,     0.85895,    0.055084,     0.17531,     0.21713,     0.87208,   -0.032221,     0.21549,     0.18821,      0.8694,   -0.032326,     0.21474,     0.18678]]),\n",
       " 'cov_prisparam': array([[    0.45348,    0.096101,    0.082763, ...,  -0.0068539,    0.041395,    0.031916],\n",
       "        [   0.096101,    0.037112,    0.021553, ...,  -0.0032338,    0.012877,   0.0095948],\n",
       "        [   0.082763,    0.021553,    0.017707, ...,  -0.0016373,   0.0089932,   0.0069435],\n",
       "        ...,\n",
       "        [ -0.0068539,  -0.0032338,  -0.0016373, ...,    0.002787,  -0.0028321, -0.00035564],\n",
       "        [   0.041395,    0.012877,   0.0089932, ...,  -0.0028321,   0.0093344,   0.0061423],\n",
       "        [   0.031916,   0.0095948,   0.0069435, ..., -0.00035564,   0.0061423,   0.0054225]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadmat(load_file_from_url(default_model_urls['niqe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/pyiqa/matlab_utils/functions.py:167: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at ../aten/src/ATen/native/Correlation.cpp:117.)\n",
      "  return torch.cov(tensor, correction=correction)\n"
     ]
    }
   ],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "import cv2\n",
    "df = pd.read_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged.csv')\n",
    "# calculate niqe for each row and add to new column\n",
    "for idx, row in df.iterrows():\n",
    "    img = cv2.imread(row['img_name'])\n",
    "    img = torch.from_numpy(img)\n",
    "    img = img.permute(2,0,1).unsqueeze(0)\n",
    "    try:\n",
    "        niqe = calculate_niqe(img, color_space='gray', pretrained_model_path='/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/model.mat')\n",
    "        df.loc[idx, 'niqe'] = niqe.item()\n",
    "    except:\n",
    "        #set NAN\n",
    "        niqe = np.nan\n",
    "        df.loc[idx, 'niqe'] = niqe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/merged_new.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2718285/2985616201.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['niqe'] = data['niqe'] / data['niqe'].max()\n"
     ]
    }
   ],
   "source": [
    "data = df.dropna()\n",
    "\n",
    "# scale niqe to [0,1]\n",
    "# data['niqe'] = (data['niqe'] - data['niqe'].min()) / (data['niqe'].max() - data['niqe'].min())\n",
    "data['niqe'] = data['niqe'] / data['niqe'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2718285/729834478.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['f2'] = data['f2'].apply(safe_convert_to_list)\n",
      "/tmp/ipykernel_2718285/729834478.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['mean_f2'] = data['f2'].apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "# Apply the conversion function to the 'f2' column and then compute the mean F2 score\n",
    "def safe_convert_to_list(s):\n",
    "    try:\n",
    "        # Attempt to directly evaluate the string\n",
    "        return ast.literal_eval(s)\n",
    "    except SyntaxError:\n",
    "        # If direct evaluation fails, attempt to manually parse the string\n",
    "        cleaned_str = s.strip('[]')\n",
    "        if cleaned_str:  # Check if the string is not empty\n",
    "            numbers = [float(num) for num in cleaned_str.split() if num not in ['[', ']']]\n",
    "            return numbers\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "data['f2'] = data['f2'].apply(safe_convert_to_list)\n",
    "data['mean_f2'] = data['f2'].apply(lambda x: np.mean(x) if len(x) > 0 else np.nan)\n",
    "\n",
    "# Prepare the dataset for modeling\n",
    "features = ['visibility', 'exposure', 'objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy', 'niqe']\n",
    "# features = ['visibility', 'exposure']\n",
    "# features = ['objectness_uncertainty', 'weighted_variance_sum', 'weighted_entropy']\n",
    "X = data[features]\n",
    "y = data['mean_f2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visibility</th>\n",
       "      <th>exposure</th>\n",
       "      <th>objectness_uncertainty</th>\n",
       "      <th>weighted_variance_sum</th>\n",
       "      <th>weighted_entropy</th>\n",
       "      <th>niqe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.083000e+03</td>\n",
       "      <td>6083.000000</td>\n",
       "      <td>6.083000e+03</td>\n",
       "      <td>6083.000000</td>\n",
       "      <td>6083.000000</td>\n",
       "      <td>6083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.777211e-01</td>\n",
       "      <td>0.231871</td>\n",
       "      <td>6.378834e-03</td>\n",
       "      <td>0.145793</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.001273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.134308e-01</td>\n",
       "      <td>0.269062</td>\n",
       "      <td>4.907808e-03</td>\n",
       "      <td>0.072428</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.024679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.222425e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.902979e-08</td>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.020507e-01</td>\n",
       "      <td>0.014917</td>\n",
       "      <td>2.376295e-03</td>\n",
       "      <td>0.096630</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.346791e-01</td>\n",
       "      <td>0.117202</td>\n",
       "      <td>5.489420e-03</td>\n",
       "      <td>0.105499</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.125931e-01</td>\n",
       "      <td>0.386221</td>\n",
       "      <td>9.104459e-03</td>\n",
       "      <td>0.209953</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.695124e-01</td>\n",
       "      <td>0.999207</td>\n",
       "      <td>2.989884e-02</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.056565</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         visibility     exposure  objectness_uncertainty  \\\n",
       "count  6.083000e+03  6083.000000            6.083000e+03   \n",
       "mean   2.777211e-01     0.231871            6.378834e-03   \n",
       "std    2.134308e-01     0.269062            4.907808e-03   \n",
       "min    3.222425e-07     0.000000            3.902979e-08   \n",
       "25%    1.020507e-01     0.014917            2.376295e-03   \n",
       "50%    2.346791e-01     0.117202            5.489420e-03   \n",
       "75%    4.125931e-01     0.386221            9.104459e-03   \n",
       "max    9.695124e-01     0.999207            2.989884e-02   \n",
       "\n",
       "       weighted_variance_sum  weighted_entropy         niqe  \n",
       "count            6083.000000       6083.000000  6083.000000  \n",
       "mean                0.145793          0.001423     0.001273  \n",
       "std                 0.072428          0.004295     0.024679  \n",
       "min                 0.050507          0.000000     0.000000  \n",
       "25%                 0.096630          0.000037     0.000033  \n",
       "50%                 0.105499          0.000158     0.000068  \n",
       "75%                 0.209953          0.000815     0.000137  \n",
       "max                 0.479202          0.056565     1.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.191435</td>\n",
       "      <td>0.100082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Regressor</th>\n",
       "      <td>0.277962</td>\n",
       "      <td>-0.306672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP Regressor</th>\n",
       "      <td>0.187570</td>\n",
       "      <td>0.118250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Regressor</th>\n",
       "      <td>0.143621</td>\n",
       "      <td>0.324849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM Regressor</th>\n",
       "      <td>0.207569</td>\n",
       "      <td>0.024235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MSE        R2\n",
       "Linear Regression        0.191435  0.100082\n",
       "Decision Tree Regressor  0.277962 -0.306672\n",
       "MLP Regressor            0.187570  0.118250\n",
       "Random Forest Regressor  0.143621  0.324849\n",
       "SVM Regressor            0.207569  0.024235"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"MLP Regressor\": MLPRegressor(random_state=42, max_iter=1000), # Increased max_iter for convergence\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
    "    # SVM\n",
    "    \"SVM Regressor\": SVR(kernel='linear', C=1.0, epsilon=0.1),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\"MSE\": mse, \"R2\": r2}\n",
    "    \n",
    "    # if name == 'Decision Tree Regressor':\n",
    "    #     # save test resulst to csv\n",
    "    #     temp = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "    #     .to_csv(f'/Data4/student_zhihan_data/source_code/IQA_A-STAR/source_code/Mydemo/test_result.csv', index=False)\n",
    "\n",
    "results_df = pd.DataFrame(results).T  # Convert results to a DataFrame for better readability\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.13651,     0.14236,      0.1451,     0.20377,     0.26094,     0.11132])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
