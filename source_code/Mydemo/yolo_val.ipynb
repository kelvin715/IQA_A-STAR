{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import yaml\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_Sharpening_0.5/test/labels.cache... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/15 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/engine/validator.py\", line 185, in __call__\n",
      "    preds = self.postprocess(preds)\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/models/yolo/detect/val.py\", line 86, in postprocess\n",
      "    return ops.non_max_suppression(\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/utils/ops.py\", line 237, in non_max_suppression\n",
      "    x = x[xc[xi]]  # confidence\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/bin/yolo\", line 8, in <module>\n",
      "    sys.exit(entrypoint())\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/cfg/__init__.py\", line 567, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/engine/model.py\", line 308, in val\n",
      "    validator(model=self.model)\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/engine/validator.py\", line 185, in __call__\n",
      "    preds = self.postprocess(preds)\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/utils/ops.py\", line 52, in __exit__\n",
      "    self.dt = self.time() - self.start  # delta-time\n",
      "  File \"/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/utils/ops.py\", line 62, in time\n",
      "    torch.cuda.synchronize(self.device)\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 783, in synchronize\n",
      "    return torch._C._cuda_synchronize()\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Exception in thread Thread-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/multiprocessing/resource_sharer.py\", line 86, in get_connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n"
     ]
    }
   ],
   "source": [
    "for i in [\"0.5\", \"1.5\", \"2.0\", \"2.5\", \"3\"]:\n",
    "    command = f'yolo task=detect mode=val model=\"/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_Sharpening_{i}_detect_by_yolov8n_with_dropblock(p=0.05/weights/best.pt\" data=\"/Data4/student_zhihan_data/data/GC10-DET_Sharpening_{i}/data.yaml\" device=0 split=\"test\"'\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_60/test/labels.cache... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        349      0.683      0.583      0.617      0.304\n",
      "                crease        230         10      0.719        0.2      0.269      0.158\n",
      "          crescent_gap        230         29      0.678      0.931      0.804      0.527\n",
      "             inclusion        230         40      0.395       0.15      0.238     0.0687\n",
      "              oil_spot        230         43      0.781      0.581        0.7      0.277\n",
      "         punching_hole        230         26      0.908      0.923      0.982      0.568\n",
      "            rolled_pit        230         10       0.17        0.1       0.17     0.0475\n",
      "             silk_spot        230         84      0.647      0.571      0.574       0.23\n",
      "         waist_folding        230         15       0.85      0.758      0.808      0.371\n",
      "            water_spot        230         42      0.857      0.712       0.77       0.47\n",
      "          welding_line        230         50      0.824        0.9      0.859       0.32\n",
      "Speed: 0.8ms preprocess, 3.1ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val88\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_120/test/labels... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 616.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_120/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        349       0.67      0.649      0.664      0.324\n",
      "                crease        230         10      0.503        0.2      0.329      0.196\n",
      "          crescent_gap        230         29      0.749      0.931      0.853      0.558\n",
      "             inclusion        230         40      0.541       0.35      0.401      0.103\n",
      "              oil_spot        230         43      0.664      0.674      0.704      0.271\n",
      "         punching_hole        230         26      0.914          1      0.995      0.549\n",
      "            rolled_pit        230         10      0.392        0.3      0.231     0.0868\n",
      "             silk_spot        230         84      0.579      0.548       0.59       0.26\n",
      "         waist_folding        230         15      0.795        0.8      0.813      0.348\n",
      "            water_spot        230         42      0.717      0.785      0.859      0.506\n",
      "          welding_line        230         50      0.851        0.9      0.866       0.36\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val89\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_180/test/labels... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 683.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_180/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        349      0.591      0.653      0.634      0.312\n",
      "                crease        230         10      0.499      0.299      0.219      0.112\n",
      "          crescent_gap        230         29      0.734      0.966      0.881      0.594\n",
      "             inclusion        230         40      0.323      0.335      0.302     0.0855\n",
      "              oil_spot        230         43      0.612      0.674      0.678      0.269\n",
      "         punching_hole        230         26      0.874          1      0.995      0.581\n",
      "            rolled_pit        230         10      0.113        0.1      0.142     0.0407\n",
      "             silk_spot        230         84      0.493      0.607      0.597      0.257\n",
      "         waist_folding        230         15       0.65        0.8      0.775      0.339\n",
      "            water_spot        230         42      0.757      0.786      0.824      0.476\n",
      "          welding_line        230         50      0.854       0.96       0.93      0.367\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val90\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_240/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_240/test/labels... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1398.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        349      0.636      0.631      0.643      0.314\n",
      "                crease        230         10      0.211        0.1      0.165     0.0995\n",
      "          crescent_gap        230         29        0.8      0.965      0.887      0.581\n",
      "             inclusion        230         40      0.455      0.225      0.306       0.09\n",
      "              oil_spot        230         43      0.652      0.605      0.696      0.279\n",
      "         punching_hole        230         26      0.943          1      0.995       0.55\n",
      "            rolled_pit        230         10       0.41        0.3      0.205     0.0723\n",
      "             silk_spot        230         84      0.614      0.619       0.64      0.285\n",
      "         waist_folding        230         15      0.651      0.733      0.772      0.307\n",
      "            water_spot        230         42      0.795      0.786      0.845      0.533\n",
      "          welding_line        230         50      0.834       0.98      0.924      0.345\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val91\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_300/test/labels... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1350.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_300/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        349      0.716      0.577      0.611      0.299\n",
      "                crease        230         10          1          0      0.107     0.0486\n",
      "          crescent_gap        230         29      0.715      0.897      0.823      0.521\n",
      "             inclusion        230         40       0.36      0.075      0.173     0.0544\n",
      "              oil_spot        230         43      0.724      0.651      0.689      0.276\n",
      "         punching_hole        230         26      0.905          1      0.995      0.587\n",
      "            rolled_pit        230         10       0.59      0.154      0.232     0.0755\n",
      "             silk_spot        230         84      0.705      0.476      0.554      0.233\n",
      "         waist_folding        230         15      0.751      0.804      0.909      0.381\n",
      "            water_spot        230         42      0.711      0.714      0.742      0.461\n",
      "          welding_line        230         50      0.702          1       0.89      0.357\n",
      "Speed: 0.8ms preprocess, 3.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val92\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "# execute linux command\n",
    "\n",
    "for i in [60,120,180,240,300]:\n",
    "    command = f'yolo task=detect mode=val model=\"/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_BilateralBlur_{i}_detect_by_yolov8n_with_dropblock(p=0.05/weights/best.pt\" data=\"/Data4/student_zhihan_data/data/GC10-DET_BilateralBlur_{i}/data.yaml\" device=0 split=\"test\"'\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_MedianBlur_43/test/labels... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1405.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_MedianBlur_43/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        349      0.688      0.534      0.545      0.253\n",
      "                crease        230         10      0.946        0.2      0.219      0.108\n",
      "          crescent_gap        230         29      0.775      0.966      0.884      0.558\n",
      "             inclusion        230         40      0.177       0.05     0.0214    0.00832\n",
      "              oil_spot        230         43      0.669      0.512       0.52      0.199\n",
      "         punching_hole        230         26      0.848      0.885      0.896      0.447\n",
      "            rolled_pit        230         10      0.445     0.0891     0.0933     0.0358\n",
      "             silk_spot        230         84      0.699      0.417      0.477       0.17\n",
      "         waist_folding        230         15      0.734        0.8      0.858      0.326\n",
      "            water_spot        230         42      0.774      0.524      0.615       0.32\n",
      "          welding_line        230         50       0.81        0.9      0.866      0.354\n",
      "Speed: 0.8ms preprocess, 3.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val103\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "for i in [43]:\n",
    "    command = f'yolo task=detect mode=val model=\"/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_MedianBlur_{i}_detect_by_yolov8n_with_dropblock(p=0.05/weights/best.pt\" data=\"/Data4/student_zhihan_data/data/GC10-DET_MedianBlur_{i}/data.yaml\" device=0 split=\"test\"'\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.0:0.05/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.0:0.05/test/labels... 230 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1342.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        347      0.639      0.624       0.62      0.302\n",
      "                crease        230         10      0.547      0.247       0.28     0.0958\n",
      "          crescent_gap        230         29      0.728      0.931      0.862      0.514\n",
      "             inclusion        230         40      0.447      0.225      0.252     0.0844\n",
      "              oil_spot        230         43      0.626      0.701      0.682      0.279\n",
      "         punching_hole        230         25      0.822       0.96      0.985      0.591\n",
      "            rolled_pit        230         10      0.469        0.3      0.299       0.11\n",
      "             silk_spot        230         84      0.586      0.607       0.59      0.249\n",
      "         waist_folding        230         15      0.642      0.717      0.696      0.288\n",
      "            water_spot        230         42      0.803      0.714      0.739      0.457\n",
      "          welding_line        230         49      0.722      0.837      0.817      0.348\n",
      "Speed: 0.8ms preprocess, 3.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val97\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.05:0.1/test/labels... 230 images, 3 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1357.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.05:0.1/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        334      0.655      0.574      0.608      0.293\n",
      "                crease        230         10      0.827        0.1      0.294      0.128\n",
      "          crescent_gap        230         27      0.637      0.889      0.857      0.476\n",
      "             inclusion        230         39      0.409      0.205      0.246     0.0729\n",
      "              oil_spot        230         39      0.679      0.538      0.618      0.287\n",
      "         punching_hole        230         22      0.947      0.811      0.824       0.48\n",
      "            rolled_pit        230          9      0.383      0.222      0.209     0.0679\n",
      "             silk_spot        230         84      0.551      0.536      0.524      0.246\n",
      "         waist_folding        230         15       0.71      0.867      0.912      0.368\n",
      "            water_spot        230         41      0.761      0.659      0.731      0.443\n",
      "          welding_line        230         48      0.643      0.917      0.866      0.366\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val98\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.1:0.15/test/labels... 230 images, 16 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1317.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.1:0.15/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        299      0.684       0.47      0.501      0.267\n",
      "                crease        230          9          1          0      0.102     0.0516\n",
      "          crescent_gap        230         26      0.571      0.846      0.734      0.452\n",
      "             inclusion        230         33      0.482      0.152       0.28     0.0823\n",
      "              oil_spot        230         33      0.493      0.515      0.405      0.161\n",
      "         punching_hole        230          8      0.746        0.5      0.578      0.313\n",
      "            rolled_pit        230          9      0.448      0.111     0.0806     0.0488\n",
      "             silk_spot        230         82      0.786      0.402      0.504      0.224\n",
      "         waist_folding        230         15      0.787       0.74      0.834      0.435\n",
      "            water_spot        230         36      0.805      0.583      0.655      0.388\n",
      "          welding_line        230         48      0.718      0.854      0.842      0.511\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val99\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.15:0.2/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.15:0.2/test/labels... 230 images, 21 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1461.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:02<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        287      0.686      0.498      0.531      0.272\n",
      "                crease        230         10      0.647        0.2       0.27      0.163\n",
      "          crescent_gap        230         24      0.606       0.75      0.695      0.331\n",
      "             inclusion        230         28          1          0      0.197     0.0762\n",
      "              oil_spot        230         34      0.637      0.621      0.632      0.241\n",
      "         punching_hole        230          6      0.636      0.667      0.642      0.387\n",
      "            rolled_pit        230         10      0.258        0.1      0.142     0.0272\n",
      "             silk_spot        230         79      0.682      0.434      0.443      0.222\n",
      "         waist_folding        230         15       0.71      0.867      0.858       0.51\n",
      "            water_spot        230         38      0.889      0.526      0.598      0.305\n",
      "          welding_line        230         43      0.797      0.814      0.837      0.459\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val100\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.2:0.25/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.2:0.25/test/labels... 230 images, 39 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1484.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        259      0.546       0.42      0.437      0.232\n",
      "                crease        230          9          0          0     0.0252      0.013\n",
      "          crescent_gap        230         23      0.397      0.522      0.421      0.241\n",
      "             inclusion        230         23      0.294      0.174        0.1     0.0314\n",
      "              oil_spot        230         26      0.243      0.231      0.253     0.0811\n",
      "         punching_hole        230          5      0.813        0.6      0.634      0.237\n",
      "            rolled_pit        230          8      0.934       0.25      0.372      0.113\n",
      "             silk_spot        230         73      0.559      0.507      0.483      0.241\n",
      "         waist_folding        230         15      0.825      0.733      0.829      0.644\n",
      "            water_spot        230         34      0.649      0.441      0.452      0.258\n",
      "          welding_line        230         43      0.747      0.744      0.803      0.456\n",
      "Speed: 0.8ms preprocess, 2.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val101\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n",
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "YOLOv8-dropblock summary: 236 layers, 3008942 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.25:0.3/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_0.25:0.3/test/labels... 230 images, 45 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 1465.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:01<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        234      0.465      0.254      0.259       0.12\n",
      "                crease        230          9          1          0     0.0151    0.00487\n",
      "          crescent_gap        230         23       0.21      0.348      0.186     0.0785\n",
      "             inclusion        230         19      0.231      0.144      0.135     0.0419\n",
      "              oil_spot        230         18      0.144      0.111     0.0597     0.0267\n",
      "         punching_hole        230          6       0.64      0.307      0.299      0.111\n",
      "            rolled_pit        230          7          1          0      0.254     0.0812\n",
      "             silk_spot        230         72      0.329      0.417      0.384      0.208\n",
      "         waist_folding        230         15      0.291      0.411      0.415       0.27\n",
      "            water_spot        230         30      0.375        0.2      0.263      0.137\n",
      "          welding_line        230         35      0.433        0.6      0.577      0.241\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1m/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/val102\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "for i in ['0.0:0.05', '0.05:0.1', '0.1:0.15', '0.15:0.2', '0.2:0.25', '0.25:0.3']:\n",
    "    command = f'yolo task=detect mode=val model=\"/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_Transform_Scale_{i}_detect_by_yolov8n_with_dropblock(p=0.05/weights/best.pt\" data=\"/Data4/student_zhihan_data/data/GC10-DET_Transform_Scale_{i}/data.yaml\" device=0 split=\"test\"'\n",
    "    os.system(command)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.2 ðŸš€ Python-3.9.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA TITAN Xp, 12187MiB)\n",
      "                                                          CUDA:1 (NVIDIA TITAN Xp, 12190MiB)\n",
      "                                                          CUDA:2 (NVIDIA TITAN Xp, 12190MiB)\n",
      "                                                          CUDA:3 (NVIDIA TITAN Xp, 12190MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3007598 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Data4/student_zhihan_data/data/GC10-DET_brightness_-10/valid/labels.cache... 456 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456/456 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         yaml\u001b[38;5;241m.\u001b[39mdump(data, file)\n\u001b[1;32m     24\u001b[0m     model \u001b[38;5;241m=\u001b[39m YOLO(model_path)  \u001b[38;5;66;03m# load a custom model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Data4/student_zhihan_data/data/GC10-DET_brightness_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m/data.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0,1,2,3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# no arguments needed, dataset and settings remembered \u001b[39;00m\n\u001b[1;32m     26\u001b[0m     records[i] \u001b[38;5;241m=\u001b[39m metrics \n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/engine/model.py:308\u001b[0m, in \u001b[0;36mModel.val\u001b[0;34m(self, validator, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[1;32m    307\u001b[0m validator \u001b[38;5;241m=\u001b[39m (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator\u001b[39m\u001b[38;5;124m\"\u001b[39m))(args\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m--> 308\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/engine/validator.py:155\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msplit), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m    154\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 155\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# warmup\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_val_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m dt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    160\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    161\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    162\u001b[0m     Profile(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    163\u001b[0m )\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/nn/autobackend.py:519\u001b[0m, in \u001b[0;36mAutoBackend.warmup\u001b[0;34m(self, imgsz)\u001b[0m\n\u001b[1;32m    517\u001b[0m im \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mimgsz, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# input\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/nn/autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    381\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/nn/tasks.py:75\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/nn/tasks.py:93\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/nn/tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    115\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/source_code/yolo/ultralytics/ultralytics/nn/modules/conv.py:73\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# return self.act(self.conv(x))\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)))\n",
      "File \u001b[0;32m/Data4/student_zhihan_data/Anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv' object has no attribute 'drop'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "records = {}\n",
    "for i in [-10, -15, -20, -25, -30, -50, -100, -150, 10, 0, 20, 30, 50, 60, 70, 90, 110]:\n",
    "    if i != 0:\n",
    "        model_path = os.path.join('/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET_brightness_%d detect by yolov8n/weights/best.pt' % i)\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "        \n",
    "        yaml_path = os.path.join('/Data4/student_zhihan_data/data/GC10-DET_brightness_%d/data.yaml' % i)\n",
    "        if not os.path.exists(yaml_path):\n",
    "            print(yaml_path)\n",
    "        \n",
    "        # Read the YAML file\n",
    "        with open(yaml_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "\n",
    "        # Append the new key-value pair\n",
    "        if 'path' not in data:\n",
    "            data['path'] = '/Data4/student_zhihan_data/data/GC10-DET_brightness_%d' % i\n",
    "        \n",
    "        # Write the modified data structure back to the YAML file\n",
    "        with open(yaml_path, 'w') as file:\n",
    "            yaml.dump(data, file)\n",
    "        \n",
    "        model = YOLO(model_path)  # load a custom model\n",
    "        metrics = model.val(data='/Data4/student_zhihan_data/data/GC10-DET_brightness_%d/data.yaml' % i, device='0,1,2,3', split='val', verbose=False)  # no arguments needed, dataset and settings remembered \n",
    "        records[i] = metrics \n",
    "    else:\n",
    "        model_path = os.path.join('/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET detect by yolov8n5/weights/best.pt')\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "        \n",
    "        yaml_path = os.path.join('/Data4/student_zhihan_data/data/GC10-DET/data.yaml')\n",
    "        if not os.path.exists(yaml_path):\n",
    "            print(yaml_path)\n",
    "        \n",
    "        # Read the YAML file\n",
    "        with open(yaml_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "\n",
    "        # Append the new key-value pair\n",
    "        if 'path' not in data:\n",
    "            data['path'] = '/Data4/student_zhihan_data/data/GC10-DET'\n",
    "        # Write the modified data structure back to the YAML file\n",
    "        with open(yaml_path, 'w') as file:\n",
    "            yaml.dump(data, file)\n",
    "        \n",
    "        model = YOLO(model_path)  # load a custom model\n",
    "        metrics = model.val(data='/Data4/student_zhihan_data/data/GC10-DET/data.yaml', device='0,1,2,3', split='val', verbose=False)  # no arguments needed, dataset and settings remembered \n",
    "        records[i] = metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([-10, -15, -20, -30, -50, -100, -150, 10, 0, 20, 30, 50, 60, 70, 90])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10 {'metrics/precision(B)': 0.6249449038647459, 'metrics/recall(B)': 0.5824428816724565, 'metrics/mAP50(B)': 0.6006457822208314, 'metrics/mAP50-95(B)': 0.2939694689322862, 'fitness': 0.32463710026114073}\n",
      "-15 {'metrics/precision(B)': 0.63540304991421, 'metrics/recall(B)': 0.557291941369644, 'metrics/mAP50(B)': 0.5786556628066502, 'metrics/mAP50-95(B)': 0.28753595728046305, 'fitness': 0.31664792783308177}\n",
      "-20 {'metrics/precision(B)': 0.6044578471210236, 'metrics/recall(B)': 0.5478730576783379, 'metrics/mAP50(B)': 0.5838985135774001, 'metrics/mAP50-95(B)': 0.290801130826919, 'fitness': 0.3201108691019671}\n",
      "-30 {'metrics/precision(B)': 0.5741561941486621, 'metrics/recall(B)': 0.5931305412688984, 'metrics/mAP50(B)': 0.5723564968996909, 'metrics/mAP50-95(B)': 0.28248264936649464, 'fitness': 0.3114700341198143}\n",
      "-50 {'metrics/precision(B)': 0.6644195087900344, 'metrics/recall(B)': 0.5700942856385729, 'metrics/mAP50(B)': 0.6035856265816145, 'metrics/mAP50-95(B)': 0.29331849781479236, 'fitness': 0.3243452106914746}\n",
      "-100 {'metrics/precision(B)': 0.5947803586621929, 'metrics/recall(B)': 0.5795931112742715, 'metrics/mAP50(B)': 0.5999287694061272, 'metrics/mAP50-95(B)': 0.2878062848652314, 'fitness': 0.319018533319321}\n",
      "-150 {'metrics/precision(B)': 0.4869671119186905, 'metrics/recall(B)': 0.3744646596479372, 'metrics/mAP50(B)': 0.38697335872609606, 'metrics/mAP50-95(B)': 0.16416541735700096, 'fitness': 0.1864462114939105}\n",
      "10 {'metrics/precision(B)': 0.534854981937438, 'metrics/recall(B)': 0.5503425273755495, 'metrics/mAP50(B)': 0.5546556558947012, 'metrics/mAP50-95(B)': 0.2699530844395611, 'fitness': 0.2984233415850751}\n",
      "0 {'metrics/precision(B)': 0.6299055138121173, 'metrics/recall(B)': 0.601450892462405, 'metrics/mAP50(B)': 0.6000783831490853, 'metrics/mAP50-95(B)': 0.30150018379405, 'fitness': 0.33135800372955354}\n",
      "20 {'metrics/precision(B)': 0.5680202401929388, 'metrics/recall(B)': 0.5490664885826269, 'metrics/mAP50(B)': 0.5496700606623386, 'metrics/mAP50-95(B)': 0.2650600213639469, 'fitness': 0.2935210252937861}\n",
      "30 {'metrics/precision(B)': 0.6231153704962735, 'metrics/recall(B)': 0.5565286366314192, 'metrics/mAP50(B)': 0.5838163798378416, 'metrics/mAP50-95(B)': 0.2886646484603545, 'fitness': 0.3181798215981032}\n",
      "50 {'metrics/precision(B)': 0.5676938201383777, 'metrics/recall(B)': 0.5701489954724703, 'metrics/mAP50(B)': 0.5587589465403755, 'metrics/mAP50-95(B)': 0.2755764159027562, 'fitness': 0.3038946689665182}\n",
      "60 {'metrics/precision(B)': 0.5778848321075294, 'metrics/recall(B)': 0.6354059758119094, 'metrics/mAP50(B)': 0.606969166457332, 'metrics/mAP50-95(B)': 0.30386058781996395, 'fitness': 0.3341714456837007}\n",
      "70 {'metrics/precision(B)': 0.6025696177934391, 'metrics/recall(B)': 0.49613325501116634, 'metrics/mAP50(B)': 0.5130623109949675, 'metrics/mAP50-95(B)': 0.24052717250438604, 'fitness': 0.2677806863534442}\n",
      "90 {'metrics/precision(B)': 0.6128944679892401, 'metrics/recall(B)': 0.5763767027267146, 'metrics/mAP50(B)': 0.5907102966612485, 'metrics/mAP50-95(B)': 0.2961961226874533, 'fitness': 0.3256475400848329}\n"
     ]
    }
   ],
   "source": [
    "for i in records:\n",
    "    print(i, records[i].results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('/Data4/student_zhihan_data/source_code/yolo/ultralytics/runs/detect/GC10-DET detect by yolov8n5/weights/best.pt')\n",
    "\n",
    "i = 0\n",
    "yaml_path = os.path.join('/Data4/student_zhihan_data/data/GC10-DET/data.yaml')\n",
    "if not os.path.exists(yaml_path):\n",
    "    print(yaml_path)\n",
    "\n",
    "# Read the YAML file\n",
    "with open(yaml_path, 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "# Append the new key-value pair\n",
    "data['path'] = '/Data4/student_zhihan_data/data/GC10-DET'\n",
    "\n",
    "# Write the modified data structure back to the YAML file\n",
    "with open('/Data4/student_zhihan_data/data/GC10-DET_brightness_-30/data.yaml', 'w') as file:\n",
    "    yaml.dump(data, file)\n",
    "    \n",
    "model = YOLO(model_path)  # load a custom model\n",
    "metrics = model.val(data='/Data4/student_zhihan_data/data/GC10-DET_brightness_%d/data.yaml' % i, device='0,1,2,3', split='val', verbose=False)  # no arguments needed, dataset and settings remembered \n",
    "records[i] = metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10 {'metrics/precision(B)': 0.674660866865132, 'metrics/recall(B)': 0.5701132068384447, 'metrics/mAP50(B)': 0.6120042593808653, 'metrics/mAP50-95(B)': 0.2997180743641319, 'fitness': 0.33094669286580525}\n",
      "-15 {'metrics/precision(B)': 0.6508055279996983, 'metrics/recall(B)': 0.5707156922718054, 'metrics/mAP50(B)': 0.5965976751415456, 'metrics/mAP50-95(B)': 0.2952428644227792, 'fitness': 0.3253783454946559}\n",
      "-20 {'metrics/precision(B)': 0.6115646182695766, 'metrics/recall(B)': 0.5578813141986456, 'metrics/mAP50(B)': 0.5977855761341153, 'metrics/mAP50-95(B)': 0.2960975543119524, 'fitness': 0.32626635649416874}\n",
      "-30 {'metrics/precision(B)': 0.5770876974552056, 'metrics/recall(B)': 0.623814996052056, 'metrics/mAP50(B)': 0.5998386724338385, 'metrics/mAP50-95(B)': 0.2960759379967164, 'fitness': 0.3264522114404286}\n",
      "-50 {'metrics/precision(B)': 0.6644195087900344, 'metrics/recall(B)': 0.5700942856385729, 'metrics/mAP50(B)': 0.6035856265816145, 'metrics/mAP50-95(B)': 0.29331849781479236, 'fitness': 0.3243452106914746}\n",
      "-100 {'metrics/precision(B)': 0.5947803586621929, 'metrics/recall(B)': 0.5795931112742715, 'metrics/mAP50(B)': 0.5999287694061272, 'metrics/mAP50-95(B)': 0.2878062848652314, 'fitness': 0.319018533319321}\n",
      "-150 {'metrics/precision(B)': 0.48166678138059255, 'metrics/recall(B)': 0.3969532036845596, 'metrics/mAP50(B)': 0.39173623656679235, 'metrics/mAP50-95(B)': 0.1671462835626861, 'fitness': 0.18960527886309672}\n",
      "10 {'metrics/precision(B)': 0.8692851940292151, 'metrics/recall(B)': 0.4396075629237476, 'metrics/mAP50(B)': 0.5249720746724105, 'metrics/mAP50-95(B)': 0.25624314505792173, 'fitness': 0.28311603801937063}\n",
      "20 {'metrics/precision(B)': 0.5315329102185066, 'metrics/recall(B)': 0.5454510889878013, 'metrics/mAP50(B)': 0.5382945370267745, 'metrics/mAP50-95(B)': 0.26289433987395205, 'fitness': 0.2904343595892343}\n",
      "30 {'metrics/precision(B)': 0.6415143231032876, 'metrics/recall(B)': 0.5784681266097652, 'metrics/mAP50(B)': 0.6039017483708441, 'metrics/mAP50-95(B)': 0.29635016822515164, 'fitness': 0.3271053262397209}\n",
      "50 {'metrics/precision(B)': 0.6360800655408384, 'metrics/recall(B)': 0.5330013591126862, 'metrics/mAP50(B)': 0.5738751512357536, 'metrics/mAP50-95(B)': 0.2714742146021708, 'fitness': 0.3017143082655291}\n",
      "60 {'metrics/precision(B)': 0.5773917165203, 'metrics/recall(B)': 0.6392890663388354, 'metrics/mAP50(B)': 0.6098038377780004, 'metrics/mAP50-95(B)': 0.3053066638804889, 'fitness': 0.3357563812702401}\n",
      "70 {'metrics/precision(B)': 0.602436784585489, 'metrics/recall(B)': 0.49210851145153905, 'metrics/mAP50(B)': 0.5086704378848104, 'metrics/mAP50-95(B)': 0.23870316460541158, 'fitness': 0.2656998919333515}\n",
      "90 {'metrics/precision(B)': 0.613115354242206, 'metrics/recall(B)': 0.5586430711421484, 'metrics/mAP50(B)': 0.5723559867457124, 'metrics/mAP50-95(B)': 0.28587436171736974, 'fitness': 0.314522524220204}\n"
     ]
    }
   ],
   "source": [
    "# records to csv\n",
    "import pandas\n",
    "columns = []\n",
    "columns.extend([i for i in records[10].results_dict.keys()])\n",
    "df = pandas.DataFrame(columns=columns)\n",
    "for i in records:\n",
    "    print(i, records[i].results_dict)\n",
    "    # add new row to df\n",
    "    df.loc[i] = records[i].results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train: ../train/images\\nval: ../valid/images\\ntest: ../test/images\\n\\nnc: 10\\nnames: ['crease', 'crescent_gap', 'inclusion', 'oil_spot', 'punching_hole', 'rolled_pit', 'silk_spot', 'waist_folding', 'water_spot', 'welding_line']\\n\\nroboflow:\\n  workspace: machinelearning-p49ei\\n  project: gc10-det-dataset\\n  version: 4\\n  license: Public Domain\\n  url: https://universe.roboflow.com/machinelearning-p49ei/gc10-det-dataset/dataset/4\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
